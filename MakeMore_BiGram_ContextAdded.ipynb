{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJEqyHbDG/STdVN9qPwheR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N534H699/MachineLearning_Learn/blob/main/MakeMore_BiGram_ContextAdded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZnWroG2uOvA"
      },
      "outputs": [],
      "source": [
        "# RECALL: Previously done with a Freq Table with Single Prev Char (then used NN)\n",
        "# BUT, if want more than one Prev Char, possible combos explodes (# Rows, e.g. 27 * 27 if 2 Char Context = 729 Possibilities... if 3 prev chars, then 27*27*27 = 19,683 possibilities of Context) -> Way too many Rows, too few Counts for each Possibility\n",
        "# SO, will implement Multi Layer Perceptron (MLP) Model -- Was known back in 2003 when Research Paper released (fairly old approach)\n",
        "# Still sticking with Characters here (not Word-Level Language Model)\n",
        "\n",
        "# Approach: Each Token will have associated Feature Vector (e.g. 30 Dimensional)\n",
        "# ... So, every (one of the e.g. 17k) Token embedded in a e.g. 30D Space (crowded, lots points for that small of space)\n",
        "# ... Starts out spread out (Tokens init randomly), Training will Tune these Embeddings of these Words via Back Propagation\n",
        "# ... I.e. These Vectors will move around in the space, Similar Tokens (e.g. Words) end up near each other (can Transfer Knowledge through the Embedding Space thanks to this, i.e. a Word next to another perhaps has similar meaning, such as \"the\" and \"a\")\n",
        "\n",
        "# Modeling Approach: Multi-Layer NN, trained via Maximizing Log Likelihood of the Training Data\n",
        "# ... Input Layer: FIRST Index into Matrix C w/ one of each Prev Tokens want take in (e.g. if 17k words, each input is INDEX of Token/Work, so input ranges 0-16999).\n",
        "# ... - Input Layer # Neurons = (1 per Prev Word * someVal) E.g. 3 Prev Words, each w/ Lookup Table corresponding to 30 cols in a Row, so 30 Neurons rep per Prev Word, total of 3 * 30 = 90 Neurons in total\n",
        "# ... - Lookup (Embedding) Table C (# Tokens x someVal, e.g. 17k x 30) -> Every Index Input is plucking out Row of Embedding Matrix C, each Index converted to the 30 Dim Vector that corresponds to the Embedding Vector for that word.\n",
        "# ... --- Shared across all the Words (always indexing into same Matrix C over and over for each Word)\n",
        "# ... - Input goes through Embedding Layer to get actual Input to NN (e.g. 3 Prev Words, 30 Neurons per Word for each entry in corresponding Row Vec, 30 * 3 = 90 Neurons in Input Layer)\n",
        "# ... Hidden Layer: Fully Connected. # Nuerons is a HyperParam can tweak. TanH Non-Linearity applied.\n",
        "# ... Output Layer: SoftMax (Probability o/p). Fully Connected Layer w/ 1 Neuron per Token (e.g. 17k Words that could come \"next\", then 17k Neurons) -> EXPENSIVE LAYER (most computation)\n",
        "# ... - Logits go in SoftMax, get exponentiated & normalized (to sum to one so have Probability Distribution for next Token in Sequence) -> And during Training we have the Label, thus can use its Index to pluck out Probability of that Token from NN Output, then we max that Token's Probability w.r.t the Params of that NN -> Optimized via Back Propagation\n",
        "# The Parameters of the NN are the Weights and Biases of the Output Layer, Hidden Layer, and Embedding Lookup Table (C) -> All these Params Optimized via Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # For Figures\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "oeKiYewr0Pol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same setup as before\n",
        "\n",
        "# This time, downloading names.txt so not need manually add\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# Read all Words in\n",
        "words = open( 'names.txt', 'r' ).read().splitlines()\n",
        "print( f\"Word Count: {len(words)}\" ); print( words[:5] ) # Verify words is populated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oImENj8W0bej",
        "outputId": "798e3361-fef3-4b37-8323-6b3912b6fcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-04 22:39:03--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-04 22:39:04 (7.37 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n",
            "Word Count: 32033\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary of Chars and Map Functions to/from Ints\n",
        "chars = sorted(list(set( ''.join(words) )))\n",
        "\n",
        "stoi = { s:i+1 for i,s in enumerate(chars) } # +1 to leave space for '.' at [0] (for ease of use and easy distinction of '.' from letters)\n",
        "stoi['.'] = 0 # Start/Stop Char\n",
        "\n",
        "itos = { i:s for s,i in stoi.items() }\n",
        "print(stoi['f'], itos[stoi['f']]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht-1wF7y0u8U",
        "outputId": "e7075f94-bd92-4cda-d252-63632bdb5159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build DataSet for Model\n",
        "\n",
        "block_size = 3 # \"Context Length\" (# Prev Chars NN take in to predict next) -- Can modify as desired for Context Length\n",
        "X, Y = [], [] # Inputs / Output\n",
        "\n",
        "#for w in words[:5]: # Start with only 5 Words\n",
        "for w in words: # All da Words\n",
        "  context = [0] * block_size # Sliding Window inits to \"...\" (i.e. Index 0)\n",
        "  for char in w + '.':\n",
        "    ix = stoi[char]\n",
        "\n",
        "    X.append( context )\n",
        "    Y.append( ix )\n",
        "\n",
        "    #print(f\"{''.join(itos[i] for i in context)} ---> {itos[ix]}\")\n",
        "    context = context[1:] + [ix] # Sliding Window shifts 1\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "print( \"X Shape/Type: \", X.shape, X.dtype )\n",
        "print( \"Y Shape/Type: \", Y.shape, Y.dtype )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Nst2KD1E8E",
        "outputId": "735312d6-2662-4035-c740-146c30266e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape/Type:  torch.Size([228146, 3]) torch.int64\n",
            "Y Shape/Type:  torch.Size([228146]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Construct Input Layer\n",
        "\n",
        "# Build Embedding Lookup Table C\n",
        "# 27 Characters, embedding in some smaller Dimensional (e.g. 17k Words/Tokens into 30 Dimensional Space)\n",
        "# For us, embed in 2D space. Each one of 27 Chars has 2D Embedding (27 x 2 Matrix C)\n",
        "# C initially randomized\n",
        "C = torch.randn( (27,2) )\n",
        "\n",
        "# NOTE: Size of Embeddings is also a Hyper Parameter\n",
        "# -- Something like only 2 Dimensional Embeddings may be too small (Neural Net isn't able use that space effectively)\n",
        "# ---- Will see this come up later, but using 2 for now\n",
        "\n",
        "# One way can Embed... Direct Index e.g. 5, C[5] -> Gives Embedding Vector for Row X\n",
        "# Previously, we took Int and did OneHot Encoding ( F.one_hot(torch.tensor(5), num_classes=27) -> Vector (27 elems) of 0s except 5th Element. 5 has to be wrapped in a Tensor)\n",
        "# If do oneHotVector.float() @ C (need .float() since oneHot is 64 Bit Int)\n",
        "# ... GET SAME RESULT if do C[5] -> I.e. Matrix Mult \"plucks out\" 5th Row (which is 1 bit in oneHotVector, rest are 0s)\n",
        "# ... THUS, can think of our (embedding of Vector) oneHotVector @ C like indexing into C AND is first Layer of a bigger Neural Network -> w/ Layer of Linear Neurons with Weight Matrix of C\n",
        "# ... We will Index for speed (not consider the oneHot interpretation)\n",
        "\n",
        "# How simultaneously embed all Examples (e.g. 32 x 3 ints in Array X, 32 examples w/ Context Length 3)?\n",
        "# ... Can INDEX with a LIST in Torch, or even w/ Tensor. E.g. C[[1,2,3]] or C[torch.tensor[1,2,3]]\n",
        "# ... Can Index w/ MultiDim Tensors of Ints. Exactly what X is...\n",
        "# ... ... So, can do C[X] -> 32x3 x 2 (orig 32x3, now w/ each entry having a 2 elem Embedding Vector)\n",
        "# ... ... E.g. X[13, 2] Example Index 13, 2nd Dim is Int 1 (tensor(1))\n",
        "# ... ... So now, E.g. C[X][13,2] -> gives Embedding for it -> e.g. tensor([ 1.0815, -0.3502] ) (still randomized)\n",
        "# ... C[5] -> 5th Row of C... C[X] -> Gets all Embeddings (Shape: [32, 3, 2])\n",
        "# ... ... SO, C[X][13,2] is indexing the first 2 Dims ([13,2] of ^), and getting that specific entry, Example Index 13 2nd Context Index\n",
        "# ... ... Which is same as C[1] (which is what X[13,2] gives, i.e. tensor(1))\n",
        "# SO can simply do:\n",
        "embedding = C[X] # X is 32x3 (32 exps, 3 context length), C[X] assoc each Char in X (each in Context too) w/ Embedding\n",
        "embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrtHQhZk6IE1",
        "outputId": "48b754b7-12ec-4dc5-b4cb-537154202b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([228146, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Construct Hidden Layer\n",
        "\n",
        "# Number Inputs to Hidden Layer is 2 Dim Embeddings per Input Layer Neuron, and 3 Neurons in Input Layer (3*2)\n",
        "# Number Neuron in Hidden Layer is up to us (e.g. 100)\n",
        "W1 = torch.randn( (3*2, 100) )\n",
        "b1 = torch.randn( 100 ) # Biases (1 per Neuron on Hidden Layer)\n",
        "\n",
        "# Want do something like: (embedding @ W1) + b1\n",
        "# BUT, Embeddings are stacked up in the Dimensions of the embedding Tensor (shape of 32x3x2 and can't mult that by 6x100)\n",
        "# Need transform 32x3x3 into 32x6 so can perform the Multiplication\n",
        "# Many ways can do this, some faster/shorter/etc. -> Torch has lots of Functions can call in Tensors (add, mult, create, concat, etc.)\n",
        "# ... torch.cat (Concatenates seq of Tensors in Given Dimension)\n",
        "# ... Can use ^ to Concatenate the 3 Embeddings for each Input\n",
        "\n",
        "# Grab all Examples, 0th Index, & all of 3rd Dimension -> Pluck out 32x2 Embeddings of just the first Token\n",
        "# Repeat for 1st Index and 2nd Index (each representing Neuron in Neural Network's Input Layer)\n",
        "\n",
        "# POOR CODE due to hard-coding Context Length (assumes 3)\n",
        "#sequenceOfTensors = [embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]]\n",
        "#torch.cat( sequenceOfTensors, 1 ) # Cat the Sequence of Tensors together along Dimension [1] (since 32x2 and want concat along the 2s Dimension)\n",
        "\n",
        "# ONE ALT (independent of Context Length):\n",
        "#torch.cat( torch.unbind(embedding, 1), 1 )  # Removes a Tensor Dimension and returns a Tuples of all Slices along a given Dimension, without it -- I.e. above we singled out the index of the [1] Dimension manually, here, we can do so for ALL in Embedding Tensor\n",
        "# -- INEFFICIENT THOUGH, Concatenation will create entirely new Memory (new Tensor w/ all new Storage since no way to Concatenate Tensors by just manipulating View Attributes)\n",
        "\n",
        "# BETTER WAY (Can re-represent a Tensor as different Size N-Dim Vectors, such as instead of a single Vector of 18 Elements, it can be made a 3x3x3 or 2x9 or 9x2 Tensor... -> Done via Views, as long as total # Elements when Mult together Dim Sizes, is the same):\n",
        "# -- EFFICIENT cuz \"Underlying Storage of Tensor\" is rep as 1 Dimensional Vector in memory, calling \"View\" manipulated some Attributes of Tensor that dictates the Dimensions of the Tensor -> NO MEMORY CHANGED/COPIED/CREATED (Storage is Identical, just Internal Attributes of View of Tensor is manipulated, such as Storage Offset, Strides, and Shape)\n",
        "##a = torch.arange(18)\n",
        "##print( a.view(3,3,2) ); # 3*3*2=18 elements still can do a.view(9,2)\n",
        "\n",
        "# So, can do this with the Emebedding Matrix (Shape of 32x3x2, can ask PyTorch view as 32x6)\n",
        "#embedding.view(32, 6) # The way this gets flattened to 32x6 causes the \"2\" Dimension to get \"Concatenated\" into single row -> SAME RESULT AS LAST \"torch.cat(...)\"\n",
        "# TEMPORARY though, only returns Tensor with the Metadata changed, not persists within \"embedding\"\n",
        "# Shape would be a 32 x 6, where the 3x2 of 32x3x2 is brought together to be a 6 element row\n",
        "\n",
        "\n",
        "# PUTTING IT ALL TOGETHER -> Can now do the Multiple we wanted to\n",
        "#testH = embedding.view(32, 6) @ W1 + b1 # WORKS (viewing 'embedding' as 32x6)\n",
        "#testH = embedding.view( embedding.shape[0], 6 ) # ^^ Still applies ,but not hard-coding 32.\n",
        "# COULD ALSO DO \"-1\" instead of \"embedding.shape[0]\", which will cause PyTorch to Infer what it must be (since # Elements must be the same, if say second Dim is 6, then it will figure out the Dimension it must be to make Multiplication work)\n",
        "#print(testH.shape) # NOW 32x100, 100, 1 for Activation for each of our 32 Examples\n",
        "\n",
        "# True for all Elements (Element-wise Equality)\n",
        "#print( embedding.view(32,6) == torch.cat(torch.unbind(embedding, 1), 1) )\n",
        "\n",
        "\n",
        "# So, all that allows us to do...\n",
        "# Broadcasting allows this (aligns on right, creates fake Dimension \"1\" (1x100 Row Vector), then copies that Row Vector Vertically for every one of the 32 Rows of the other -> I.e. Same Bias Vector Added to all Rows of the Matrix, which is what we want)\n",
        "# 32, 100\n",
        "#  1, 100\n",
        "h = torch.tanh( embedding.view(-1, 6) @ W1 + b1 ) # Using \"-1\" insead of of embedding.shape[0] -> So PyTorch will Infer Size based on what'll make Matrix/Vector Multiplication Work\n"
      ],
      "metadata": {
        "id": "IqXVm-pwcuFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Construct Output Layer\n",
        "W2 = torch.randn( 100, 27 ) # Input 100, Output # Neurons = 27 (since 27 possible Characters that come next)\n",
        "b2 = torch.randn( 27 )\n",
        "\n",
        "\n",
        "logits = h @ W2 + b2 # Currently, with our first 5 Words, 32 x 27\n",
        "\n",
        "############### Can be replaced w/ Cross Entropy ###############\n",
        "# As before, exponentiate logits to get \"Fake Counts\", and continue w/ SoftMax\n",
        "counts = logits.exp()\n",
        "probabilities = counts / counts.sum(1, keepdims=True) # Along 1st Dimension\n",
        "print(probabilities.shape) # (originally) 32 x 27 (numExamples x posNextChar)\n",
        "#probabilities[0].sum() # Each Row Sums to 1 (i.e. now Normalized)\n",
        "\n",
        "exampleCount = X.shape[0]\n",
        "loss = -probabilities[torch.arange(exampleCount), Y].log().mean()\n",
        "################################################################\n",
        "\n",
        "# NOTE: Instead of `logits` through `loss` above, could use Cross Entropy (combines )\n",
        "# - FAR MORE EFFICIENT. Since no Intermediate Tensors created (all .log().mean() create new Tensors in Memory)\n",
        "# -- PyTorch clusters up all these Ops and often use Fused Kernels to efficiently eval these Expressions (Clustered Mathematical Operations)\n",
        "# -- Analyticly and Mathematically it's a simpler Backward Pass to calculate (and thus implement)\n",
        "# --- E.g. (e^(2*x) - 1) / (e^(2*x) + 1) far more complicated Mathematical Expression than say (1 - t^2)\n",
        "# -- Cross Entropy tends be more well-behaved (Mathematically)\n",
        "# ---- E.g. logits = tensor([-2,3,0,5]), fine, but what if tensor([-100, -3, 0, 5]) -- More extreme values (possible during optimization of NN)\n",
        "# ------ Because logits.exp() (which is e^somePow) will cause issues if have +100 in ^ due to logits.exp() (neg causes 1 / e^somePow = fine) Tensor containing an \"inf\" because run out of range in Floating Point Number (e^100 -> \"inf\" val due to out of range can represent #)\n",
        "# ------ PyTorch sovles this by offset Logits by some (same) arbitrary constant (any offset shared, produces same probabilities ultimately) -> PyTorch subtracts the Max Value, which makes largest # 0, and all else Negative, so exp() behaves appropriately (only causes issues on large Positive #s)\n",
        "# Cross Entropy finds difference between 2 Distributions\n",
        "# - Distribution of Model Predictions (logits)\n",
        "# - Distribution of Training Data (Y)\n",
        "loss = F.cross_entropy( logits, Y )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL-XpxzPOxaq",
        "outputId": "f73cc913-fa04-4347-a1c1-ed46549f3727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146, 27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== SIMPLIFIED ======================\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # For Consistency/Reproducibility\n",
        "\n",
        "# Input Layer\n",
        "C = torch.randn( (27,2), generator=g )\n",
        "embedding = C[X]\n",
        "\n",
        "\n",
        "# Hidden Layer\n",
        "hiddenLayerNeuronCount = 100\n",
        "W1 = torch.randn( (3*2, hiddenLayerNeuronCount), generator=g )\n",
        "b1 = torch.randn( hiddenLayerNeuronCount, generator=g )\n",
        "\n",
        "h = torch.tanh( embedding.view(-1, 3*2) @ W1 + b1 ) # E.g. (32, 100)\n",
        "\n",
        "\n",
        "# Output Layer\n",
        "W2 = torch.randn( (hiddenLayerNeuronCount, 27), generator=g )\n",
        "b2 = torch.randn( 27, generator=g )\n",
        "\n",
        "logits = h @ W2 + b2 # 32 x 27\n",
        "#counts = logits.exp()\n",
        "#probabilities = counts / counts.sum( 1, keepdims=True )\n",
        "#loss = -probabilities[ torch.arange(32), Y ].log().mean()\n",
        "loss = F.cross_entropy( logits, Y )\n",
        "\n",
        "print( loss )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdIW2De4j3l",
        "outputId": "709a709d-ab10-4bf0-8760-2715676a4831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(19.5052)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# =================================== RESET ===================================\n",
        "# =============================================================================\n",
        "# Quick Re-Init of Model Parameters\n",
        "\n",
        "C = torch.randn( (27,2), generator=g )\n",
        "\n",
        "hiddenLayerNeuronCount = 100 # HyperParam\n",
        "W1 = torch.randn( (3*2, hiddenLayerNeuronCount), generator=g ) # Input = hiddenLayerNeuronCount, Output = 3*2 (3 Neurons, each with 2 Embedded Values)\n",
        "b1 = torch.randn( hiddenLayerNeuronCount )\n",
        "\n",
        "W2 = torch.randn( (hiddenLayerNeuronCount, 27), generator=g )\n",
        "b2 = torch.randn( 27, generator=g )\n",
        "\n",
        "\n",
        "# Ensure all Params Require Gradient (to be calc by Torch) -- Repeating here for Convenience.\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "bBlCTHu_R571"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== Training Loop ========================\n",
        "# NOTE: ReRun above Block of Code to reinit all Parameters\n",
        "\n",
        "# Save Parameters to loop over later (e.g. set Grad to None)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "print(f\"Parameter Count: {sum(p.nelement() for p in parameters)}\") # 3481 Parameters\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "\n",
        "#for i in range(1000): # Takes a while cuz on FULL DataSet\n",
        "for i in range(10):\n",
        "\n",
        "  embeddings = C[X]\n",
        "\n",
        "  h = torch.tanh( embeddings.view(-1, 3*2) @ W1 + b1 )\n",
        "\n",
        "  logits = h @ W2 + b2\n",
        "\n",
        "  loss = F.cross_entropy( logits, Y )\n",
        "  print( f\"Loss on Iteration {i} = {loss}\" )\n",
        "\n",
        "  # Reset\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward() # Populate the Gradients\n",
        "\n",
        "  # Update\n",
        "  for p in parameters:\n",
        "    p.data += -0.1 * p.grad\n",
        "\n",
        "# Will be SMALL if not many Examples, cuz easy to Overfit (only 32 Examples but 3481 Params))\n",
        "# Can't be exactly 0 cuz multiple Examples have same Target (e.g. ...->e, ...->o, ...->a, ...->i, ...->s  i.e. multiple possibilities to follow Context=\"...\")\n",
        "# Loss is very low or 0 for cases that have unique Input for Unique Ouptut, such as \"oli\"->v, which appears once in first 5 Examples\n",
        "print(f\"Final Loss = {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSMdbK_i74L1",
        "outputId": "86dff00c-625d-41ae-8d16-7de58f19e3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Count: 3481\n",
            "Loss on Iteration 0 = 14.742026329040527\n",
            "Loss on Iteration 1 = 13.952454566955566\n",
            "Loss on Iteration 2 = 13.301827430725098\n",
            "Loss on Iteration 3 = 12.680068969726562\n",
            "Loss on Iteration 4 = 12.080190658569336\n",
            "Loss on Iteration 5 = 11.605718612670898\n",
            "Loss on Iteration 6 = 11.283839225769043\n",
            "Loss on Iteration 7 = 11.156293869018555\n",
            "Loss on Iteration 8 = 10.670614242553711\n",
            "Loss on Iteration 9 = 10.348132133483887\n",
            "Final Loss = 10.348132133483887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes a WHILE since we're going over entire DataSet EVERY ITERATION (and tanH w/ Matrix Multiplication is on large Matrices, and all the Back Propagation being done, with large amounts of Data)\n",
        "# In Practice, we tend use Mini Batches instead, where we Train on a Mini Batches only (subset of Training Set)\n",
        "# Forwarding / Backwarding LOTS of Examples (~228k Examples)\n",
        "\n",
        "# SO, randomly select some Portion of DataSet, use as Mini Batch (each Iteration)\n",
        "# Only Forward / Backward / Update on that Mini Batch, then iterate on the Mini Batches\n",
        "\n",
        "# NOTE: Mini Batch Gradient less reliable (direction not necessarily actual Gradient Direction)\n",
        "# ... Mini Batch Gradient is less Quality than Exact Gradient\n",
        "# ... BUT, this works well in practice.\n",
        "# ... Better have an Approx Gradient and take more Steps, than evaluate Exact Gradient and take fewer Steps\n"
      ],
      "metadata": {
        "id": "Aydhz-LGsM3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== Training Loop ========================\n",
        "# ======================= WITH MINI BATCH ========================\n",
        "# NOTE: ReRun above Parameter Block of Code to reinit all Parameters\n",
        "\n",
        "# Save Parameters to loop over later (e.g. set Grad to None)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "print(f\"Parameter Count: {sum(p.nelement() for p in parameters)}\") # 3481 Parameters\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "\n",
        "miniBatchSize = 32\n",
        "\n",
        "\n",
        "for i in range(1000): # FASTER now cuz using single Mini Batch\n",
        "\n",
        "  # Construct Mini Batch\n",
        "  # Range 0 to however many Examples there are (i.e. serves as Index into X)\n",
        "  # And there's miniBatchSize many of them\n",
        "  ix = torch.randint( 0, X.shape[0], (miniBatchSize,) ) # 32 Elements\n",
        "\n",
        "\n",
        "  # Forward Pass\n",
        "\n",
        "  # Grab Mini Batch (by indexing X w/ ix to get only miniBatchSize many Rows of X)\n",
        "  embeddings = C[X[ix]] # Now (32, 3, 2) -> 32 Examples in Mini Batch\n",
        "\n",
        "  h = torch.tanh( embeddings.view(-1, 3*2) @ W1 + b1 )\n",
        "\n",
        "  logits = h @ W2 + b2\n",
        "\n",
        "  loss = F.cross_entropy( logits, Y[ix] ) # Mini Batch Index into Y to get associated Ys\n",
        "  #print( f\"Loss on Iteration {i} = {loss}\" )\n",
        "\n",
        "\n",
        "  # Backward Pass\n",
        "\n",
        "  # Reset\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward() # Populate the Gradients\n",
        "\n",
        "  # Update\n",
        "  for p in parameters:\n",
        "    p.data += -0.1 * p.grad # NOT CLEAR IF 0.1 is good Learning rate -> SEE NEXT SECTION BELOW\n",
        "\n",
        "# Will be SMALL if not many Examples, cuz easy to Overfit (only 32 Examples but 3481 Params))\n",
        "# Can't be exactly 0 cuz multiple Examples have same Target (e.g. ...->e, ...->o, ...->a, ...->i, ...->s  i.e. multiple possibilities to follow Context=\"...\")\n",
        "# Loss is very low or 0 for cases that have unique Input for Unique Ouptut, such as \"oli\"->v, which appears once in first 5 Examples\n",
        "\n",
        "# Due to Mini Batch, loss here only reflects last Mini Batch ...\n",
        "#print(f\"Final Loss = {loss}\")\n",
        "\n",
        "\n",
        "# FINALLY...\n",
        "# ... Want the Loss on whole Training Set, so recalculate that\n",
        "embedding = C[X]\n",
        "h = torch.tanh( embedding.view(-1, 3*2) @ W1 + b1 )\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy( logits, Y )\n",
        "print(f\"Final Loss = {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4e5690-770c-4dba-ec23-7784334564bd",
        "id": "7Otexd3brJmk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Count: 3481\n",
            "Final Loss = 2.708108901977539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== Learning Rate Adjustment ==========================\n",
        "\n",
        "'''\n",
        "How Determine Reasonable Learning Rate\n",
        "\n",
        "A) Trial/Error\n",
        "- Set iterations (# Steps) to something fairly low, not too low (e.g. 100)\n",
        "-- Print Loss at each Step\n",
        "- Find a reasonable Search Range (e.g. Learning Rate between -0.001 and -1)\n",
        "-- Done by trying Learning Rates, e.g. -0.0001 at first\n",
        "-- Each Step, check if Loss decreasing sufficiently (not too slow)\n",
        "---- Find the Learning Rate that consistently decreases Loss (but not quickly) -> THAT IS LOWER BOUND\n",
        "---- Find Learning Rate where it decreases fast (but doesn't \"explode\", meaning may go from 70 to 60 then 90 Loss again) -> UPPER BOUND\n",
        "---- THESE are LOWER and UPPER Bounds\n",
        "-- Reset all Params (e.g. Weights) after each Trial\n",
        "\n",
        "- Use torch.linspace( lower, upper, numberOfSteps ) to get 1000 #s between lower and upper (linearly)\n",
        "-- E.g. torch.linspace( 0.001, 1, 1000 ) -> 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, ... 0.170, 0.171, ...\n",
        "-- Not make sense to step between lower/upper linearly BECAUSE(?) want bias towards upper end (???)\n",
        "-- SO:\n",
        "--- learningRateExp = torch.linspace( -3, 0, 1000 ) -> 10 ^ -3 = 0.001 and 10 ^ 0 = 1\n",
        "--- learningRates = 10 ** learningRateExp\n",
        "--- RESULT: learningRates are spread out EXPONENTIALLY in Interval [lower, upper]\n",
        "\n",
        "'''\n",
        "\n",
        "# Learning Rate Sampling Now Applied\n",
        "learningRateExponents = torch.linspace( -3, 0, 1000 )\n",
        "learningRates = 10 ** learningRateExponents\n",
        "\n",
        "\n",
        "# Variables to track stats for later reflection\n",
        "learningRates_i = []\n",
        "learningRateExponents_i = []\n",
        "loss_i = []\n",
        "\n",
        "MINI_BATCH_SIZE = 32\n",
        "\n",
        "for i in range(1000): # Number Steps corresponds to Number Learning Rates\n",
        "\n",
        "  # Create Mini Batch\n",
        "  ix = torch.randint( 0, X.shape[0], (MINI_BATCH_SIZE,) )\n",
        "\n",
        "  embeddings = C[X[ix]] # 32x3 x 2\n",
        "\n",
        "\n",
        "  # Hidden Layer\n",
        "  h = torch.tanh( embeddings.view(-1, 3*2) @ W1 + b1 ) # 32 x 100\n",
        "\n",
        "\n",
        "  # Output Layer\n",
        "  logits = h @ W2 + b2 # 32 x 27\n",
        "  loss = F.cross_entropy( logits, Y[ix] );\n",
        "  print( loss.item() )\n",
        "\n",
        "\n",
        "  # Backward Propagation\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  learningRate = learningRates[i]\n",
        "  for p in parameters:\n",
        "    p.data += -learningRate * p.grad\n",
        "\n",
        "\n",
        "  # Track Stats for later Plotting\n",
        "  learningRates_i.append( learningRate )\n",
        "  learningRateExponents_i.append( learningRateExponents[i] )\n",
        "  loss_i.append( loss.item() )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkY5uW4AzGtT",
        "outputId": "1e93a228-695e-4113-f9e0-c6c020057d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5679523944854736\n",
            "2.9284276962280273\n",
            "2.983466386795044\n",
            "2.614595651626587\n",
            "2.585796594619751\n",
            "2.4752495288848877\n",
            "2.8201937675476074\n",
            "2.885814666748047\n",
            "2.5642459392547607\n",
            "2.5582613945007324\n",
            "2.5855941772460938\n",
            "2.622565269470215\n",
            "2.2134814262390137\n",
            "2.5761189460754395\n",
            "2.3638997077941895\n",
            "2.4927196502685547\n",
            "3.07482647895813\n",
            "2.721318483352661\n",
            "2.3579747676849365\n",
            "2.2258832454681396\n",
            "3.088027000427246\n",
            "3.336595058441162\n",
            "2.654057264328003\n",
            "2.533184766769409\n",
            "2.5678675174713135\n",
            "2.8380463123321533\n",
            "2.78670334815979\n",
            "2.6585259437561035\n",
            "2.794635772705078\n",
            "2.871967315673828\n",
            "2.7765238285064697\n",
            "2.8395583629608154\n",
            "2.617410659790039\n",
            "2.7927017211914062\n",
            "2.6649622917175293\n",
            "3.1293723583221436\n",
            "2.6890275478363037\n",
            "2.6245038509368896\n",
            "2.385080575942993\n",
            "3.092694044113159\n",
            "2.6276090145111084\n",
            "2.571617841720581\n",
            "2.525383949279785\n",
            "2.7415997982025146\n",
            "2.574552297592163\n",
            "2.2509987354278564\n",
            "2.5930943489074707\n",
            "2.4673244953155518\n",
            "2.724018096923828\n",
            "2.554866075515747\n",
            "2.990338087081909\n",
            "2.6822457313537598\n",
            "2.638437032699585\n",
            "2.5009329319000244\n",
            "2.8496499061584473\n",
            "2.4506847858428955\n",
            "2.9718575477600098\n",
            "2.944247007369995\n",
            "2.2152209281921387\n",
            "2.8604180812835693\n",
            "2.9057247638702393\n",
            "3.132873773574829\n",
            "3.0114474296569824\n",
            "2.8689892292022705\n",
            "2.552213430404663\n",
            "2.6293017864227295\n",
            "2.5352399349212646\n",
            "2.5181031227111816\n",
            "2.6365926265716553\n",
            "2.6194162368774414\n",
            "2.4946961402893066\n",
            "2.7586944103240967\n",
            "2.6738319396972656\n",
            "2.9105746746063232\n",
            "3.167538642883301\n",
            "2.912353038787842\n",
            "2.545548677444458\n",
            "2.4543216228485107\n",
            "2.977835178375244\n",
            "3.082219362258911\n",
            "2.5031676292419434\n",
            "2.4293930530548096\n",
            "2.219320297241211\n",
            "2.401972770690918\n",
            "2.850616693496704\n",
            "2.5410125255584717\n",
            "2.7522003650665283\n",
            "2.3013267517089844\n",
            "2.292632579803467\n",
            "2.961338996887207\n",
            "2.6183815002441406\n",
            "2.498213052749634\n",
            "2.3879005908966064\n",
            "2.520839214324951\n",
            "2.793844699859619\n",
            "2.3362958431243896\n",
            "2.3937132358551025\n",
            "2.7353577613830566\n",
            "2.411372661590576\n",
            "2.4222288131713867\n",
            "2.7887563705444336\n",
            "2.695305109024048\n",
            "2.818767786026001\n",
            "2.719022750854492\n",
            "2.5031936168670654\n",
            "2.573603630065918\n",
            "2.479797840118408\n",
            "2.430769443511963\n",
            "2.4804608821868896\n",
            "2.7523040771484375\n",
            "2.643671751022339\n",
            "3.1268484592437744\n",
            "2.341980218887329\n",
            "2.4242730140686035\n",
            "2.7590770721435547\n",
            "2.750779867172241\n",
            "2.701967239379883\n",
            "2.795771360397339\n",
            "2.5473458766937256\n",
            "2.6558918952941895\n",
            "2.4790689945220947\n",
            "2.731332302093506\n",
            "2.3940224647521973\n",
            "2.2022924423217773\n",
            "2.5320334434509277\n",
            "2.841386556625366\n",
            "2.5866036415100098\n",
            "2.6353871822357178\n",
            "2.5902743339538574\n",
            "2.755002498626709\n",
            "2.4199609756469727\n",
            "2.400542736053467\n",
            "2.522691011428833\n",
            "2.5640530586242676\n",
            "2.6708154678344727\n",
            "2.744612455368042\n",
            "2.7006139755249023\n",
            "2.8152644634246826\n",
            "2.43068790435791\n",
            "2.7575771808624268\n",
            "2.4831831455230713\n",
            "2.535515308380127\n",
            "2.5871741771698\n",
            "2.5430421829223633\n",
            "2.3243191242218018\n",
            "2.5850720405578613\n",
            "2.9180150032043457\n",
            "2.4979801177978516\n",
            "2.66345477104187\n",
            "2.6160125732421875\n",
            "2.8459155559539795\n",
            "2.533095121383667\n",
            "2.566364049911499\n",
            "2.9636621475219727\n",
            "3.0423524379730225\n",
            "2.562974691390991\n",
            "2.445343255996704\n",
            "2.160475730895996\n",
            "2.5451908111572266\n",
            "2.8120994567871094\n",
            "2.462538719177246\n",
            "3.002091407775879\n",
            "2.949882984161377\n",
            "2.6361489295959473\n",
            "2.742145538330078\n",
            "3.146178960800171\n",
            "2.956662178039551\n",
            "2.2818715572357178\n",
            "2.8973193168640137\n",
            "2.67989444732666\n",
            "2.4792819023132324\n",
            "2.6437699794769287\n",
            "2.3580567836761475\n",
            "2.4633147716522217\n",
            "2.672882556915283\n",
            "2.3378562927246094\n",
            "2.67582368850708\n",
            "2.606398820877075\n",
            "2.767301559448242\n",
            "2.5215351581573486\n",
            "2.1848156452178955\n",
            "2.572732448577881\n",
            "2.3597867488861084\n",
            "2.8475639820098877\n",
            "2.5679056644439697\n",
            "2.661085605621338\n",
            "2.401414394378662\n",
            "2.699007034301758\n",
            "2.9451634883880615\n",
            "2.6681933403015137\n",
            "2.5995075702667236\n",
            "2.63637638092041\n",
            "2.4693918228149414\n",
            "2.634763240814209\n",
            "2.6955230236053467\n",
            "2.5706164836883545\n",
            "3.157431125640869\n",
            "2.941270589828491\n",
            "2.7836029529571533\n",
            "2.521143913269043\n",
            "2.7526912689208984\n",
            "2.674147129058838\n",
            "2.407270908355713\n",
            "2.0066022872924805\n",
            "2.5487778186798096\n",
            "2.7069551944732666\n",
            "2.387786865234375\n",
            "3.038883686065674\n",
            "2.934699535369873\n",
            "2.8859479427337646\n",
            "2.7016637325286865\n",
            "2.6368298530578613\n",
            "2.238527297973633\n",
            "2.327824354171753\n",
            "3.011220932006836\n",
            "2.7055273056030273\n",
            "2.8576223850250244\n",
            "2.8604843616485596\n",
            "2.4952282905578613\n",
            "2.772688150405884\n",
            "2.98713755607605\n",
            "2.4793574810028076\n",
            "2.5301244258880615\n",
            "3.378962755203247\n",
            "2.411809206008911\n",
            "2.5879647731781006\n",
            "2.2287821769714355\n",
            "2.4552793502807617\n",
            "2.887613534927368\n",
            "2.3818089962005615\n",
            "2.892270565032959\n",
            "2.6087327003479004\n",
            "3.049088478088379\n",
            "2.8258612155914307\n",
            "2.7136027812957764\n",
            "2.418860912322998\n",
            "3.1788840293884277\n",
            "2.467350482940674\n",
            "2.2140018939971924\n",
            "2.8177645206451416\n",
            "2.455690383911133\n",
            "2.6732747554779053\n",
            "2.916260242462158\n",
            "2.832444429397583\n",
            "2.3687374591827393\n",
            "2.9857029914855957\n",
            "2.346419095993042\n",
            "2.5772736072540283\n",
            "2.4370269775390625\n",
            "2.4866998195648193\n",
            "2.745692253112793\n",
            "2.362231969833374\n",
            "2.9612457752227783\n",
            "3.039403200149536\n",
            "2.554102897644043\n",
            "2.5240910053253174\n",
            "2.3806681632995605\n",
            "2.702666997909546\n",
            "2.5423426628112793\n",
            "2.618680477142334\n",
            "2.2120254039764404\n",
            "2.708962917327881\n",
            "2.3345508575439453\n",
            "2.505140542984009\n",
            "2.7559139728546143\n",
            "2.743056297302246\n",
            "2.614086151123047\n",
            "2.7618563175201416\n",
            "2.7346549034118652\n",
            "2.6470484733581543\n",
            "2.840672492980957\n",
            "2.3471741676330566\n",
            "2.48240327835083\n",
            "2.44474458694458\n",
            "2.9915826320648193\n",
            "2.5516927242279053\n",
            "2.3603196144104004\n",
            "2.489525318145752\n",
            "2.4786252975463867\n",
            "2.8025290966033936\n",
            "2.767235040664673\n",
            "2.50333309173584\n",
            "2.387169361114502\n",
            "2.55668568611145\n",
            "2.449411630630493\n",
            "2.9405393600463867\n",
            "2.5651681423187256\n",
            "2.505018949508667\n",
            "2.564390182495117\n",
            "2.2854042053222656\n",
            "2.8017313480377197\n",
            "2.346184015274048\n",
            "2.425422191619873\n",
            "2.8128347396850586\n",
            "2.5120012760162354\n",
            "2.4660351276397705\n",
            "2.8076183795928955\n",
            "2.5739946365356445\n",
            "2.788442611694336\n",
            "1.8459657430648804\n",
            "2.848501682281494\n",
            "3.3092761039733887\n",
            "2.858218193054199\n",
            "2.947791576385498\n",
            "2.546443223953247\n",
            "2.442723035812378\n",
            "3.0014286041259766\n",
            "2.2771682739257812\n",
            "2.536783218383789\n",
            "2.3672876358032227\n",
            "2.73675274848938\n",
            "2.640191078186035\n",
            "2.5745530128479004\n",
            "2.7477450370788574\n",
            "2.659207820892334\n",
            "2.320406436920166\n",
            "2.6726572513580322\n",
            "2.8041329383850098\n",
            "2.461728811264038\n",
            "2.5457711219787598\n",
            "2.595669746398926\n",
            "2.4909305572509766\n",
            "2.3474743366241455\n",
            "2.475759983062744\n",
            "2.700164318084717\n",
            "2.7743804454803467\n",
            "2.53591251373291\n",
            "2.6960010528564453\n",
            "2.682988166809082\n",
            "2.395218849182129\n",
            "2.6347508430480957\n",
            "2.289369821548462\n",
            "2.6738953590393066\n",
            "2.5286269187927246\n",
            "2.6127591133117676\n",
            "2.992777109146118\n",
            "2.661236047744751\n",
            "2.453334093093872\n",
            "2.539149045944214\n",
            "3.020812749862671\n",
            "2.5209579467773438\n",
            "2.7698140144348145\n",
            "2.8238699436187744\n",
            "2.8046610355377197\n",
            "2.3887624740600586\n",
            "2.564282178878784\n",
            "2.646991491317749\n",
            "2.6042990684509277\n",
            "2.6295387744903564\n",
            "2.853339910507202\n",
            "2.8177034854888916\n",
            "2.328272581100464\n",
            "2.5651485919952393\n",
            "2.512218952178955\n",
            "2.4621994495391846\n",
            "2.475390911102295\n",
            "2.55576229095459\n",
            "2.524144172668457\n",
            "2.1950690746307373\n",
            "2.384876012802124\n",
            "2.671269416809082\n",
            "2.508774518966675\n",
            "2.555006980895996\n",
            "2.790673017501831\n",
            "2.4308600425720215\n",
            "2.3382253646850586\n",
            "2.4943134784698486\n",
            "2.3982067108154297\n",
            "2.875422477722168\n",
            "2.759714126586914\n",
            "2.38621187210083\n",
            "3.0437870025634766\n",
            "2.467698097229004\n",
            "2.49694561958313\n",
            "2.509589433670044\n",
            "2.6836044788360596\n",
            "2.635097026824951\n",
            "2.5271120071411133\n",
            "2.200812816619873\n",
            "2.7227561473846436\n",
            "2.6573166847229004\n",
            "2.467510938644409\n",
            "2.733003616333008\n",
            "2.554659128189087\n",
            "2.798391342163086\n",
            "2.675813913345337\n",
            "2.6645009517669678\n",
            "2.482639789581299\n",
            "2.578772783279419\n",
            "2.5368902683258057\n",
            "2.606318950653076\n",
            "2.6172099113464355\n",
            "2.6850852966308594\n",
            "2.7028865814208984\n",
            "2.55082631111145\n",
            "2.758368730545044\n",
            "2.5357656478881836\n",
            "2.440690040588379\n",
            "2.2993292808532715\n",
            "2.9852325916290283\n",
            "2.66508150100708\n",
            "3.0152695178985596\n",
            "3.089778423309326\n",
            "2.2520570755004883\n",
            "2.594331979751587\n",
            "2.7974181175231934\n",
            "3.0852506160736084\n",
            "2.5956308841705322\n",
            "2.348956346511841\n",
            "2.4633095264434814\n",
            "2.302649736404419\n",
            "2.9068000316619873\n",
            "2.3639602661132812\n",
            "2.6100783348083496\n",
            "2.7901055812835693\n",
            "2.7902019023895264\n",
            "2.495333433151245\n",
            "2.8496735095977783\n",
            "2.3734374046325684\n",
            "2.959778070449829\n",
            "2.4812309741973877\n",
            "2.256356716156006\n",
            "2.6055290699005127\n",
            "2.3295507431030273\n",
            "3.0199666023254395\n",
            "3.0340983867645264\n",
            "2.449192762374878\n",
            "2.8383777141571045\n",
            "2.222745895385742\n",
            "2.9105842113494873\n",
            "2.5426199436187744\n",
            "2.5012874603271484\n",
            "2.6083028316497803\n",
            "3.2391793727874756\n",
            "2.9363038539886475\n",
            "3.0950067043304443\n",
            "3.0130136013031006\n",
            "3.128782272338867\n",
            "2.7211742401123047\n",
            "2.3056929111480713\n",
            "2.8244314193725586\n",
            "2.707780361175537\n",
            "2.8364944458007812\n",
            "2.579455852508545\n",
            "2.377519130706787\n",
            "2.5115411281585693\n",
            "2.243222236633301\n",
            "2.6417791843414307\n",
            "2.798252820968628\n",
            "2.5343596935272217\n",
            "2.6672472953796387\n",
            "2.8817858695983887\n",
            "2.593574047088623\n",
            "2.773780107498169\n",
            "2.9219443798065186\n",
            "2.480120897293091\n",
            "2.4821295738220215\n",
            "2.5044729709625244\n",
            "2.42411732673645\n",
            "2.661953926086426\n",
            "2.9959161281585693\n",
            "2.5730929374694824\n",
            "2.3963942527770996\n",
            "2.3115389347076416\n",
            "2.623565196990967\n",
            "2.734870672225952\n",
            "2.7250897884368896\n",
            "2.5918338298797607\n",
            "2.466656446456909\n",
            "2.659396171569824\n",
            "3.049248456954956\n",
            "2.892270088195801\n",
            "2.278437852859497\n",
            "2.6616358757019043\n",
            "2.8221683502197266\n",
            "2.5619497299194336\n",
            "2.3374812602996826\n",
            "2.6589276790618896\n",
            "2.4893784523010254\n",
            "2.3697221279144287\n",
            "3.2409493923187256\n",
            "2.565178871154785\n",
            "2.255737781524658\n",
            "2.8313207626342773\n",
            "2.8708863258361816\n",
            "2.5171754360198975\n",
            "2.5656802654266357\n",
            "2.767460584640503\n",
            "2.6629433631896973\n",
            "2.7194154262542725\n",
            "2.5400948524475098\n",
            "2.8503949642181396\n",
            "3.0153818130493164\n",
            "2.4263317584991455\n",
            "2.4522528648376465\n",
            "2.8347158432006836\n",
            "2.7925002574920654\n",
            "2.750213623046875\n",
            "2.3476390838623047\n",
            "2.6040828227996826\n",
            "2.836613178253174\n",
            "2.5171005725860596\n",
            "2.4819703102111816\n",
            "2.260316848754883\n",
            "2.7685165405273438\n",
            "2.7126665115356445\n",
            "3.021292209625244\n",
            "2.5535993576049805\n",
            "2.4531025886535645\n",
            "2.5551910400390625\n",
            "2.726621389389038\n",
            "2.4317822456359863\n",
            "2.4201700687408447\n",
            "2.8656845092773438\n",
            "2.2671241760253906\n",
            "2.3673975467681885\n",
            "2.357081890106201\n",
            "2.2949652671813965\n",
            "2.9091880321502686\n",
            "2.344780206680298\n",
            "2.6027071475982666\n",
            "2.836566209793091\n",
            "2.3676695823669434\n",
            "2.50604248046875\n",
            "2.616468667984009\n",
            "2.732004404067993\n",
            "2.770777940750122\n",
            "2.3893744945526123\n",
            "2.21917462348938\n",
            "2.575173854827881\n",
            "2.6379053592681885\n",
            "2.7317469120025635\n",
            "2.275930643081665\n",
            "2.464874029159546\n",
            "2.7264599800109863\n",
            "2.4399330615997314\n",
            "2.6593079566955566\n",
            "2.8364038467407227\n",
            "2.7062816619873047\n",
            "2.335315227508545\n",
            "2.593930959701538\n",
            "2.784562110900879\n",
            "2.5031795501708984\n",
            "2.9585213661193848\n",
            "2.304795503616333\n",
            "2.676360845565796\n",
            "2.6596429347991943\n",
            "2.5369138717651367\n",
            "2.512122631072998\n",
            "2.635871410369873\n",
            "2.597966194152832\n",
            "2.9289004802703857\n",
            "2.4478282928466797\n",
            "2.539994716644287\n",
            "2.878019094467163\n",
            "2.446516752243042\n",
            "2.484023332595825\n",
            "2.5414984226226807\n",
            "2.427879810333252\n",
            "2.4636154174804688\n",
            "3.0599453449249268\n",
            "2.506662607192993\n",
            "2.4377212524414062\n",
            "2.8829715251922607\n",
            "2.521716356277466\n",
            "2.325122356414795\n",
            "2.892671585083008\n",
            "2.651456594467163\n",
            "2.384042263031006\n",
            "3.0724387168884277\n",
            "2.5262091159820557\n",
            "2.536919116973877\n",
            "2.732802629470825\n",
            "2.8296027183532715\n",
            "2.3156871795654297\n",
            "2.920686721801758\n",
            "2.60781192779541\n",
            "2.5233113765716553\n",
            "2.974708080291748\n",
            "2.3885016441345215\n",
            "2.6187713146209717\n",
            "2.438584566116333\n",
            "2.8011934757232666\n",
            "3.052788019180298\n",
            "2.6286065578460693\n",
            "2.4568562507629395\n",
            "3.2581419944763184\n",
            "2.6673378944396973\n",
            "2.823896646499634\n",
            "2.4472506046295166\n",
            "2.606560468673706\n",
            "2.6899499893188477\n",
            "2.520536422729492\n",
            "2.760314464569092\n",
            "2.813826322555542\n",
            "2.4365952014923096\n",
            "2.158918857574463\n",
            "2.6222307682037354\n",
            "2.675076961517334\n",
            "2.6942293643951416\n",
            "2.895296812057495\n",
            "2.9410877227783203\n",
            "2.6275618076324463\n",
            "2.5122814178466797\n",
            "2.3547401428222656\n",
            "3.0663270950317383\n",
            "2.4929654598236084\n",
            "2.5361900329589844\n",
            "2.878146171569824\n",
            "2.7136037349700928\n",
            "2.4340639114379883\n",
            "2.794750690460205\n",
            "2.7312021255493164\n",
            "2.4685280323028564\n",
            "2.4133059978485107\n",
            "2.5951011180877686\n",
            "2.6809253692626953\n",
            "2.555224657058716\n",
            "2.4634335041046143\n",
            "2.550865888595581\n",
            "2.425105333328247\n",
            "2.5191125869750977\n",
            "2.809814453125\n",
            "2.2555930614471436\n",
            "2.418426275253296\n",
            "2.616122245788574\n",
            "2.9014718532562256\n",
            "2.9396328926086426\n",
            "2.491827964782715\n",
            "2.701655864715576\n",
            "2.216566801071167\n",
            "2.798002004623413\n",
            "2.8590099811553955\n",
            "2.6511526107788086\n",
            "3.061061382293701\n",
            "2.6715879440307617\n",
            "2.566200017929077\n",
            "2.9493868350982666\n",
            "3.1237802505493164\n",
            "2.8634283542633057\n",
            "2.776073694229126\n",
            "2.7882726192474365\n",
            "2.7485947608947754\n",
            "2.442866086959839\n",
            "2.898435115814209\n",
            "2.735954761505127\n",
            "3.0172109603881836\n",
            "2.8590526580810547\n",
            "2.6494252681732178\n",
            "2.8292813301086426\n",
            "3.2085788249969482\n",
            "2.5864789485931396\n",
            "2.4323830604553223\n",
            "2.87058424949646\n",
            "2.7217354774475098\n",
            "2.7860071659088135\n",
            "2.6887333393096924\n",
            "2.4523298740386963\n",
            "2.6502878665924072\n",
            "2.695702075958252\n",
            "2.6864893436431885\n",
            "2.3023815155029297\n",
            "2.4449405670166016\n",
            "2.550175189971924\n",
            "2.0931098461151123\n",
            "2.803896427154541\n",
            "2.589707136154175\n",
            "2.5294342041015625\n",
            "2.7373502254486084\n",
            "3.22257661819458\n",
            "3.1524007320404053\n",
            "2.4416158199310303\n",
            "2.58005952835083\n",
            "2.5330166816711426\n",
            "2.453777551651001\n",
            "2.8643763065338135\n",
            "2.1403987407684326\n",
            "2.8243987560272217\n",
            "2.5924301147460938\n",
            "2.45546555519104\n",
            "2.6512324810028076\n",
            "2.936298131942749\n",
            "2.847214460372925\n",
            "2.9175665378570557\n",
            "2.3546547889709473\n",
            "2.2995002269744873\n",
            "2.806851387023926\n",
            "2.337038516998291\n",
            "2.845928907394409\n",
            "2.7648768424987793\n",
            "2.709010601043701\n",
            "3.09448504447937\n",
            "2.6313138008117676\n",
            "2.932722568511963\n",
            "2.4110729694366455\n",
            "2.3251757621765137\n",
            "2.6243956089019775\n",
            "2.9036965370178223\n",
            "2.6017844676971436\n",
            "2.9561028480529785\n",
            "2.7797348499298096\n",
            "2.964456081390381\n",
            "2.91220760345459\n",
            "2.485013246536255\n",
            "2.6241843700408936\n",
            "2.8330655097961426\n",
            "2.634385108947754\n",
            "2.8598175048828125\n",
            "2.2485878467559814\n",
            "2.532716751098633\n",
            "2.883241653442383\n",
            "3.6930720806121826\n",
            "2.590285062789917\n",
            "2.7847635746002197\n",
            "2.4545257091522217\n",
            "3.08498477935791\n",
            "3.4152488708496094\n",
            "2.4645864963531494\n",
            "2.8231565952301025\n",
            "2.9201295375823975\n",
            "2.6873507499694824\n",
            "3.029561758041382\n",
            "2.8907790184020996\n",
            "2.3934326171875\n",
            "2.694018602371216\n",
            "2.7728383541107178\n",
            "2.7824361324310303\n",
            "2.485083818435669\n",
            "3.1398541927337646\n",
            "3.3584039211273193\n",
            "3.117868661880493\n",
            "3.3248579502105713\n",
            "3.037062644958496\n",
            "2.807312488555908\n",
            "2.7796707153320312\n",
            "2.8117012977600098\n",
            "3.5737438201904297\n",
            "3.403930425643921\n",
            "2.8002939224243164\n",
            "2.8516361713409424\n",
            "2.7550363540649414\n",
            "2.4676313400268555\n",
            "3.3257553577423096\n",
            "3.3570454120635986\n",
            "3.0849995613098145\n",
            "2.9122893810272217\n",
            "2.4849853515625\n",
            "3.156127452850342\n",
            "3.2882134914398193\n",
            "2.9102210998535156\n",
            "3.3819947242736816\n",
            "2.7225935459136963\n",
            "2.5400564670562744\n",
            "2.564263105392456\n",
            "2.8862826824188232\n",
            "2.929260015487671\n",
            "3.095729112625122\n",
            "2.640369176864624\n",
            "2.954436779022217\n",
            "2.807818651199341\n",
            "3.42948579788208\n",
            "2.823549747467041\n",
            "2.5795602798461914\n",
            "2.894479990005493\n",
            "3.0937981605529785\n",
            "2.9405691623687744\n",
            "2.357787609100342\n",
            "2.7564170360565186\n",
            "3.014152765274048\n",
            "3.475553035736084\n",
            "3.042182445526123\n",
            "3.1988325119018555\n",
            "3.6665000915527344\n",
            "2.8552393913269043\n",
            "3.5633769035339355\n",
            "3.3764684200286865\n",
            "3.3452839851379395\n",
            "2.91447114944458\n",
            "3.1299116611480713\n",
            "2.703328847885132\n",
            "2.6598851680755615\n",
            "3.521456718444824\n",
            "3.1097354888916016\n",
            "3.8498265743255615\n",
            "3.901571035385132\n",
            "3.209537982940674\n",
            "2.9565231800079346\n",
            "3.4142403602600098\n",
            "3.011033535003662\n",
            "3.431443929672241\n",
            "3.0414178371429443\n",
            "3.123779773712158\n",
            "2.715907096862793\n",
            "3.178251266479492\n",
            "3.4447977542877197\n",
            "3.1481237411499023\n",
            "3.763805627822876\n",
            "2.765627145767212\n",
            "2.773794651031494\n",
            "2.733562469482422\n",
            "3.5730888843536377\n",
            "3.3732411861419678\n",
            "3.136046886444092\n",
            "2.6717910766601562\n",
            "3.142476797103882\n",
            "2.4699349403381348\n",
            "3.061692237854004\n",
            "2.987475633621216\n",
            "3.3436315059661865\n",
            "2.8119986057281494\n",
            "3.072795867919922\n",
            "3.674043893814087\n",
            "3.6993284225463867\n",
            "4.067239284515381\n",
            "3.012213706970215\n",
            "3.7284610271453857\n",
            "3.5328123569488525\n",
            "3.3722245693206787\n",
            "3.5617787837982178\n",
            "3.0610435009002686\n",
            "3.572845935821533\n",
            "2.5838913917541504\n",
            "3.160356044769287\n",
            "3.8499984741210938\n",
            "4.212411403656006\n",
            "2.8691983222961426\n",
            "3.8066670894622803\n",
            "3.856659173965454\n",
            "4.771207332611084\n",
            "3.1748530864715576\n",
            "3.2957592010498047\n",
            "3.2422096729278564\n",
            "2.8254857063293457\n",
            "3.688851833343506\n",
            "3.854464292526245\n",
            "5.323651313781738\n",
            "3.640233039855957\n",
            "3.1883132457733154\n",
            "3.2595202922821045\n",
            "2.8144726753234863\n",
            "2.9691295623779297\n",
            "3.213376998901367\n",
            "3.9778201580047607\n",
            "4.439091682434082\n",
            "3.9484567642211914\n",
            "3.7289953231811523\n",
            "3.260904312133789\n",
            "3.845345973968506\n",
            "5.646682262420654\n",
            "5.79898738861084\n",
            "4.945271968841553\n",
            "4.605565071105957\n",
            "5.207476615905762\n",
            "4.433653354644775\n",
            "3.2888174057006836\n",
            "4.400874137878418\n",
            "3.1297354698181152\n",
            "3.6240360736846924\n",
            "4.3431477546691895\n",
            "3.681302070617676\n",
            "4.169774055480957\n",
            "4.7849955558776855\n",
            "4.095944404602051\n",
            "4.738642692565918\n",
            "3.914299249649048\n",
            "3.989738702774048\n",
            "4.409807205200195\n",
            "4.371269226074219\n",
            "4.416280746459961\n",
            "4.5488409996032715\n",
            "3.6512739658355713\n",
            "3.526580333709717\n",
            "3.448646306991577\n",
            "3.9945435523986816\n",
            "5.170711517333984\n",
            "5.004857540130615\n",
            "4.364641189575195\n",
            "4.383922100067139\n",
            "4.104637622833252\n",
            "4.521758079528809\n",
            "6.588779449462891\n",
            "5.152717113494873\n",
            "3.685188055038452\n",
            "3.6591293811798096\n",
            "4.906235218048096\n",
            "3.5481820106506348\n",
            "4.617880344390869\n",
            "4.569503307342529\n",
            "5.9846086502075195\n",
            "3.8219757080078125\n",
            "4.119167804718018\n",
            "4.303286075592041\n",
            "3.6920154094696045\n",
            "3.6677908897399902\n",
            "5.041442394256592\n",
            "7.487710952758789\n",
            "5.086135387420654\n",
            "6.026915073394775\n",
            "5.5184125900268555\n",
            "4.353301048278809\n",
            "5.024614334106445\n",
            "3.862926959991455\n",
            "5.102071762084961\n",
            "4.759593486785889\n",
            "7.536850452423096\n",
            "5.289764881134033\n",
            "6.793449878692627\n",
            "4.768136978149414\n",
            "4.375106334686279\n",
            "4.542926788330078\n",
            "5.461791038513184\n",
            "4.86346435546875\n",
            "7.198523044586182\n",
            "5.036308288574219\n",
            "4.203373432159424\n",
            "5.494709014892578\n",
            "6.022857189178467\n",
            "5.6431965827941895\n",
            "4.283607482910156\n",
            "4.338532447814941\n",
            "4.962731838226318\n",
            "4.751226425170898\n",
            "6.189509868621826\n",
            "6.8753790855407715\n",
            "7.140047550201416\n",
            "5.853109359741211\n",
            "6.250409126281738\n",
            "4.646629333496094\n",
            "6.1028056144714355\n",
            "5.411393165588379\n",
            "5.250452518463135\n",
            "5.5741167068481445\n",
            "5.768022537231445\n",
            "4.611873626708984\n",
            "7.324162483215332\n",
            "4.781325817108154\n",
            "4.711818695068359\n",
            "4.630985736846924\n",
            "4.358986854553223\n",
            "6.943235397338867\n",
            "4.842401504516602\n",
            "5.261396884918213\n",
            "3.866959810256958\n",
            "5.738978385925293\n",
            "5.413948059082031\n",
            "7.70920467376709\n",
            "7.052479267120361\n",
            "6.168071746826172\n",
            "7.333658695220947\n",
            "6.959596633911133\n",
            "6.193634986877441\n",
            "6.961156845092773\n",
            "7.6395368576049805\n",
            "7.326782703399658\n",
            "7.513338565826416\n",
            "8.99448013305664\n",
            "6.412282466888428\n",
            "5.089311599731445\n",
            "5.238523006439209\n",
            "6.617044925689697\n",
            "7.463964462280273\n",
            "7.194734573364258\n",
            "7.098733901977539\n",
            "6.704639434814453\n",
            "6.262022972106934\n",
            "6.485491752624512\n",
            "8.514082908630371\n",
            "6.5105791091918945\n",
            "6.120391368865967\n",
            "6.78740930557251\n",
            "7.026206970214844\n",
            "7.805016040802002\n",
            "7.413801670074463\n",
            "5.019083023071289\n",
            "5.662935256958008\n",
            "6.65162992477417\n",
            "6.338161468505859\n",
            "7.496419429779053\n",
            "8.891291618347168\n",
            "5.723973274230957\n",
            "5.682806015014648\n",
            "6.167155742645264\n",
            "6.311014175415039\n",
            "6.210080146789551\n",
            "7.139693260192871\n",
            "6.869577407836914\n",
            "7.8349504470825195\n",
            "13.540498733520508\n",
            "10.13279914855957\n",
            "11.880067825317383\n",
            "11.414705276489258\n",
            "4.8754353523254395\n",
            "8.870355606079102\n",
            "6.47114896774292\n",
            "6.459102153778076\n",
            "6.6761794090271\n",
            "8.263959884643555\n",
            "9.100586891174316\n",
            "7.645781993865967\n",
            "10.580023765563965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows Learning Rates we used on X Axis, then Y Axis has the Loss we observed w/ that Learning Rate\n",
        "# COMMON PATTERN\n",
        "# At first, very low Learning Rate, gradually decreased Loss as increased Learning rate\n",
        "# Got to a good spot around 0.1, then as increased Learning Rate even more, Loss blew up\n",
        "# After a certain point of increasing Learning Rate, the Loss Exploded (became very unstable)\n",
        "plt.plot( learningRates_i, loss_i ) # Learning Rate only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Thd8MtU0BIry",
        "outputId": "542c673f-15fc-419a-a2ac-583349214de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f938d8a8760>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot( learningRateExponents_i, loss_i ) # Learning Rate Exponents\n",
        "\n",
        "# This shows which Exponent of Learning Rate to use, somewhere around min / valley\n",
        "# For our setup, shows around -1 is good Exponent (10 ^ -1 = 0.1, which is our original we used, which was good)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "4xP_ePGfCoVs",
        "outputId": "3e5ad541-5ff4-4c92-c0ee-0535ade9c838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f938d284760>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGfCAYAAAD/BbCUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdOUlEQVR4nO3dd3xT9foH8E9Gm7bQwYaWFspW9pA9BRmignr1ioiKXhHFgVv0ugeuiwp6nVfQ30VcV0RRBCxThbJl71VG2XSPNDm/P9Kk33NyTnKSJmmaft6vV7U9OUlO05DznOf7fJ+vQZIkCUREREQhYqzqAyAiIqKahcEHERERhRSDDyIiIgopBh9EREQUUgw+iIiIKKQYfBAREVFIMfggIiKikGLwQURERCHF4IOIiIhCisEHERERhZTZ1zusWrUKb775JjZu3IiTJ09i/vz5GDt2rOq+kydPxkcffYS3334bU6dO1fX4drsdJ06cQHx8PAwGg6+HR0RERFVAkiTk5eUhOTkZRqPn3IbPwUdBQQE6d+6MO+64A9ddd53mfvPnz8fatWuRnJzs0+OfOHECqampvh4WERERhYGsrCw0bdrU4z4+Bx+jRo3CqFGjPO5z/Phx3H///Vi8eDFGjx7t0+PHx8cDcBx8QkKCr4dHREREVSA3Nxepqamu87gnPgcf3tjtdkyYMAGPPfYY2rdv73X/kpISlJSUuH7Oy8sDACQkJDD4ICIiqmb0lEwEvOD09ddfh9lsxgMPPKBr/+nTpyMxMdH1xSEXIiKiyBbQ4GPjxo149913MWfOHN3FotOmTUNOTo7rKysrK5CHRERERGEmoMHH6tWrcfr0aaSlpcFsNsNsNuPIkSN45JFH0Lx5c9X7WCwW1xALh1qIiIgiX0BrPiZMmIBhw4bJto0YMQITJkzAxIkTA/lUREREVE35HHzk5+dj//79rp8PHTqELVu2oG7dukhLS0O9evVk+0dFRaFx48Zo27Zt5Y+WiIiIqj2fg48NGzZgyJAhrp8ffvhhAMBtt92GOXPmBOzAiIiIKDL5HHwMHjwYkiTp3v/w4cO+PgURERFFMK7tQkRERCHF4IOIiIhCisEHERERhRSDDyIiIgopBh9EREQUUgw+iIiIItDRc4X4aOUB5JeUVfWhuAn4qrZERERU9Ua+uwqFpTYcPleA6dd1qurDkWHmg4iIKAIVltoAAJkHz1fxkbhj8EFERBTB9LcFDR0GH0RERBHMl67kocLgg4iIiEKKwQcREVEEC7+8B4MPIiIiCjEGH0RERBRSDD6IiIgiWBjWmzL4ICIiimRSGFZ9MPggIiKikGLwQURERCHF4IOIiCiCseaDiIiIQorBBxEREdV4DD6IiIgopBh8EBERUUgx+CAiIqKQYvBBREQUwaQwrDhl8EFERBTBwi/0YPBBREREIcbgg4iIiEKKwQcREVEEC8OSDwYfREREFFoMPoiIiCKYFIYlpww+iIiIIhiHXYiIiKjGY/BBREREIcXgg4iIiEKKwQcREVEEC8OSDwYfREREkYwFp0RERFTjMfggIiKKaOGX+mDwQURERCHF4IOIiIhCisEHERFRBGPBKREREYVUGMYeDD6IiIgotBh8EBERUUgx+CAiIopgUhgWffgcfKxatQpXX301kpOTYTAY8MMPP7hus1qteOKJJ9CxY0fUqlULycnJuPXWW3HixIlAHjMRERHpFH6hhx/BR0FBATp37oz333/f7bbCwkJs2rQJzzzzDDZt2oTvv/8ee/bswTXXXBOQgyUiIqLqz+zrHUaNGoVRo0ap3paYmIilS5fKtr333nvo2bMnjh49irS0NP+OkoiIiPwShqMuwa/5yMnJgcFgQFJSUrCfioiIiKoBnzMfviguLsYTTzyBcePGISEhQXWfkpISlJSUuH7Ozc0N5iERERFRFQta5sNqteLGG2+EJEn44IMPNPebPn06EhMTXV+pqanBOiQiIqIaJyJmu+jhDDyOHDmCpUuXamY9AGDatGnIyclxfWVlZQXjkIiIiGqk8As9gjDs4gw89u3bh+XLl6NevXoe97dYLLBYLIE+DCIiIgKQV1yG8wWlqFsruqoPxcXnzEd+fj62bNmCLVu2AAAOHTqELVu24OjRo7Barfjb3/6GDRs2YO7cubDZbMjOzkZ2djZKS0sDfexERESkw5uL91T1Icj4nPnYsGEDhgwZ4vr54YcfBgDcdttteP755/Hjjz8CALp06SK73/LlyzF48GD/j5SIiIj8cja/xPtOIeRz8DF48GCPxSvhWNhCRERUk1lt9qo+BBmu7UJERBThSssYfBAREVEIMfNBREREIVVqC6+SCAYfREREEY7DLkRERBRSpWW2qj4EGQYfREREEc7KYRciIiIKJRacEhERUUix5oOIiIhCqpSZDyIiIgolDrsQERFRSLHglIiIiELKUNUHoMDgg4iIKMIZDeEVfjD4ICIiinThFXsw+CAiIop0RgYfREREFEocdiEiIqKQYvBBREREIRVeoQeDDyIiosgXZtEHgw8iIqIIx2EXIiIiCinOdiEiIqKQMjDzQURERKHEzAcRERGFFDMfREREFFLMfBAREVFIGcJsri2DDyIiogjHzAcRERGFFGs+iIiIKKTCLPZg8EFERBTp2OGUiIiIQoo1H0RERBRSrPkgIiKikAqz2APmqj4AIiIiCi5nzcexC4X4en0W6taKxsR+6VV3PFX2zERERBQSkiQBAI5fKMKsZfvx37VHqvR4GHwQERHVEHZHDFLlNSAMPoiIiCKc5Pq/47uqLgFh8EFERFRTlEchVd33g8EHERFRpCsPOiqGXaruUAAGH0RERBFPOexS1Rh8EBER1RASh12IiIgoFJxTbe3l/+ewCxEREQWVpPg/Mx9EREQUEhIzH0RERBQKzloP5//Z54OIiIhCQmKHUyIiIgoF5xTbaltwumrVKlx99dVITk6GwWDADz/8ILtdkiQ8++yzaNKkCWJjYzFs2DDs27cvUMdLREREPnINu5T/XO2GXQoKCtC5c2e8//77qre/8cYbmDlzJj788ENkZmaiVq1aGDFiBIqLiyt9sERERKRPQozZbVu49PlwPzIvRo0ahVGjRqneJkkS3nnnHfzzn//EmDFjAABffPEFGjVqhB9++AE33XRT5Y6WiIiIdBF7mVYUnFbTYRdPDh06hOzsbAwbNsy1LTExEb169cKaNWtU71NSUoLc3FzZFxEREQVexbBLBBWcZmdnAwAaNWok296oUSPXbUrTp09HYmKi6ys1NTWQh0RERFQzqSzjUm0LTgNt2rRpyMnJcX1lZWVV9SERERFVe/JhF6n8/46fIyr4aNy4MQDg1KlTsu2nTp1y3aZksViQkJAg+yIiIqLAi8j26unp6WjcuDEyMjJc23Jzc5GZmYk+ffoE8qmIiIjIA2e2AxDWdgmTYRefZ7vk5+dj//79rp8PHTqELVu2oG7dukhLS8PUqVPx8ssvo3Xr1khPT8czzzyD5ORkjB07NpDHTURERB6oz3Zx/L+qC059Dj42bNiAIUOGuH5++OGHAQC33XYb5syZg8cffxwFBQWYNGkSLl68iP79++PXX39FTExM4I6aiIiIPJJUCk6dnU6rXeZj8ODBslSOksFgwIsvvogXX3yxUgdGREREgeFqr253/My1XYiIiCjgJJW5ttW2vToRERGFP3GQQtnh1BhJU22JiIgo/FTMdnH8n8MuREREFHBq1ZnOoRhmPoiIiCjwVIZd7K5tzHwQERFRgMkLTiO4vToRERGFLw67EBERUdCozXaxh0mHUwYfREREEUi1HWiYrO3C4IOIiCjCSYr/R9SqtkRERBQeZKvaSs726q5xlyrF4IOIiCgCqff5cGDmg4iIiILKGXSESeKDwQcREVEkUluAXmLBKREREYWCMhDhsAsREREFlKSINlwFp87MR8iPSI7BBxERUQ3hikk47EJERESBpBxmYZ8PIiIiCiq3WlMJKC2z47VFuwFUeeKDwQcREVFN8OeBs67vmfkgIiKigHIrOAVgs1ds41RbIiIiUmW3S5jwn0w8+u1fPt1PrbupEHsw+CAiIiJ1O07kYvW+s/hu4zGf7udWcCpJiswHh12IiIhIRZndHpDHkSAfimHBKREREalSGz7R8sWaw+j/+jIcOVcASXHPwlIbLhZZXT9z2IWIiIhUicMnyiJSpWcX7MCxC0V48aedquu6TPt+m+rjVgUGH0RERGGrIkqw6wwYynTsWMWxB4MPIiKicGX3IfPhZDJ6H1Nh5oOIiIhUiUGC3syHAd6DC72BTLAw+CAiIgpTYpCgLCLVYjAYvO7LzAcRERGpEmMEvQGDjlEX3YFMsDD4ICIiClPy2S767mM0GHQMu/h/TIHA4IOIiChMSbLZLnqHXbzPZtFbPxIsDD6IiIjClaT6rUd6VqxlwSkRERGpEkMEMfNRWFqGMpt663Wj0eA1uGCfDyIiIlIlq/kojzXyS8pw6bOLMWzGStX7GHUMuzDzQURERKrsKlNtNx+9AAA4fK5Q9T56lm1hzQcRERGpkg+76LuPrtkufh9RYDD4ICIiClOyJmO6Z7t4H3fhsAsRERGp8qe9uqPmgwWnRERE5Aeb3b3mwxtOtSUiIiK/yQpO9WY+jN73tavP0g0ZBh9ERERhylvwoZbBcCws5xnXdiEiIiJVYh8xtfbqagGJnqm2XNuFiIiIVMn7fHi+3ckx1dZzdME+H0RERKRKDC7sKhGDWhChp8NpVc93CXjwYbPZ8MwzzyA9PR2xsbFo2bIlXnrppSqvrCUiIqpuZLNdVE6japkPg54mY1V8SjYH+gFff/11fPDBB/j888/Rvn17bNiwARMnTkRiYiIeeOCBQD8dERFRxPI21dYZRIgX+Lqm2lb+0Col4MHHn3/+iTFjxmD06NEAgObNm2PevHlYt25doJ+KiIgoonlrMubMfIi36WkyppYxCaWAD7v07dsXGRkZ2Lt3LwDgr7/+wu+//45Ro0YF+qmIiIgims1Le3VnECFmSIw6ij6quuA04JmPJ598Erm5uWjXrh1MJhNsNhteeeUVjB8/XnX/kpISlJSUuH7Ozc0N9CERERFVS2JQoZ75cP6/4kZ9U20jLPPxzTffYO7cufjyyy+xadMmfP7553jrrbfw+eefq+4/ffp0JCYmur5SU1MDfUhERETVkjxIUKv5cM986GkyVtUCHnw89thjePLJJ3HTTTehY8eOmDBhAh566CFMnz5ddf9p06YhJyfH9ZWVlRXoQyIiIqqW9GY+bLKCUx3t1as48xHwYZfCwkIYjfKYxmQywa7RSN5iscBisQT6MIiIiKo9mxAjVMxsqdjmqvmwicGHwfuqtpFW83H11VfjlVdeQVpaGtq3b4/NmzdjxowZuOOOOwL9VERERBHNLst8SLL/i9+LmQ8dM20jL/Mxa9YsPPPMM7j33ntx+vRpJCcn4+6778azzz4b6KciIiKKaGqBhhg3fLUuC0fPF+KR4W1c2yTJe2Yj4jIf8fHxeOedd/DOO+8E+qGJiIhqFPlUW8f/xYBkxlJHW4vWDWtX7AdJViuipqoLUrm2CxERURgRZ7jYVdqrq8UVp/NKZPuVeQs+Im2qLREREflnwZbj6PbSUqw/fB6APNBwFpGq1WuUlNlc39slwFY+ySPKpF4AUtXDLgw+iIiIwsSDX23BhUIr7pyzHoD6VFu1rEWJtWJGqQQJ1vLZL2aj+mmewy5EREQkU2qzY+ORC3g3Y59rm+Sa7eK+f3GZ0M5CqghazBqZj4SYgJd8+oTBBxERUZix2SU8+NVm2Ta1VupOJdaKYRcJFTUfUSb10/xLYzsE5kD9xOCDiIiokmx2KaBFnFabpFKXoZ35KBEyH3a75Kr5MBndMx//ua0HmtaJC9Sh+oXBBxERUSWUltkx8I3lGP9pZkAf1xIlP0V7qvkoVmY+yms+olSCD6PKtlBj8EFEROQDSZIwM2MfFu/IBgBsybqI4xeL8OeBcwF9nmjFkIlae3UnMfMhyWo+3E/zRj0tUIOsaitOiIiIqpk/D5xzNfc6/NrooD2PJcok+/n1X3cjp8iKCb2bue1bKg67SJKr5kOt4NTE4IOIiKh6OZVbLPs5WOdyiyJrsfHIBQDA538edttXHHYBgDJnnw+VqbZhMOrCYRciIqJwpKz5cMottrptyy8pc30vSZKr5kMt88GaDyIiIvKJGGg4ie3UJQg1H2oFp2Ew7MLgg4iIKEB8mW67fM9pDHpzOTaUt1JXWr3vrOr2YqGbqZPVplXz4X6a12j9EVJhcAhERETVl5hH8KXVx8TZ63HkXGFApujaFAvQecp8GJj5ICIiihxq3Ue9EafJ+stZ4wF473AaDrNdGHwQERFVgngut0uOIZA1B865zUAJJqtd3uejrHwYRrXglMEHERFR5LBLEl75eRfGfbIWj3+31ef7q7VD10NMuEhizYdqh1O/niKgwuAQiIiIIsec8j4cP/51Qrb909UH8c2GLI/39Tf4EEkS8GXmUQCAWbXPR9VnPthkjIiIKEC0aj4OnS3Ayz/vAgDc2CNV8/5RRgNKK3kMVrsdO0/mAgDO5pe43R6IAKeymPkgIiKqlIqTudqKs4C8K6pdayeoT431lVUoPlVrSBYGsQeDDyIiokDRynwUlVYUn9o8zIhRq9HwlVWYPSMGIk7hMOzC4IOIiChAtOIKsSupzUPmIxCtz8uEmS+lKtN4GXwQERFVc+K5XKvDaWFpRfChlh1x3k+tB4ev2RAx2yF2PnUKh5oPFpwSERH5SRlsaCU1CkqEYReVnXpPz8CtfZqrBgbRZiPKSvX3DBEDjiKVXiNhkPhg5oOIiMhfyiSGVs2HLPOh0tD0VG4J3ly8RzUwiDb7dqoWgw+1RmfhkPlg8EFEROQnCfLsh1bwUaCz4FQtMLD4HHxIqt+7niMMUh8MPoiIiPxklyR59kMjrhAzEJ4KTtUCg8pkPl4a28Htdi4sR0REVI3ZJUlW5/HT1pOq+4kLv3lafE5ttku0j70/xOBjQu9mbrdz2IWIiKgakyT5sMtLC3eq7icGBL5nPkw+HZMz0GkQb1G9PQxiDwYfRERE/pIk7RkuolKdwUcgCk6dzxWlEWUEopdIZTH4ICIi8pNdkiBpFXoIxGEXz8GHSsGpn8MuJpNG8MGaDyIioupLcv3HM9mwi4eaD7UmZb5mPpyBjtqKtgBnuxAREVU74rlbWXCqRQw+PC0spxaX+DvbRaszakxU1Z/6q/4IiIiIwtDMjH14+OstbtkI8UfJDl3DLqXisIuHzIfaTBhfZ6c4e3to3Y9TbYmIiMLUjKV78f3m49h45ILmPhJ0Zj7K9BWcqgUfvtaHujIfKjUftaJ9mzkTLAw+iIiIPCi2yvuhy4ddtBeTE8mHXbT3U3soA3yLPsrs2jUfCbFRPj1WsDD4ICIi8kA5rCIGCG4dTjXoLjhV2aZRN+qVWs1HIoMPIiKi8Oe+eJz8Np9rPnwcdvE18+GkVvORkhTr12MFGoMPIiIiD5ThgDjMIkmSx2EUpzKdTcZUgw8hhvBlpkqUSn+QGy9L1X3/YGLwQURE5IGn2S52SVebD93t1dUCGbEpWG2LWcezOSgzH5MHtcSI9o113z+YGHwQEREpeCoiFbMTjtkuegpO9S0sp0YMIuKi9QcfWn0+wgGDDyIiIgVZXYfiNkmxn55YQu/aLupTbSuCiFo+ZD6UwqC9hwuDDyIiIgVZEOBWcCpkMeyS+w4qdA+7qDYZq/i+tkV/n46Ve8/o3jfUGHwQEREpKIdW5Lcp9/X+eGKTsYlz1nt4Xvdt/g67lOk5sCrC4IOIiEjB41CKJK/f0NfnQ18goPZYsoLTGDNev74j6teO1vV4ojAadWHwQUREpCQbdfGQ6bBL+gpIxZoPz8/reW0Xo8GAv1+Whpt7pnl9rOeuvlTXc1YFBh9EREQKsmEXDzUfkuS9xZinVWw9PW9a3Tj88sAAWebDuVyLWaWHh6hHszqY2C9dti3iC06PHz+OW265BfXq1UNsbCw6duyIDRs2BOOpiIiIAs6uGFoRufX58JL5UGun/mXmUY3ndfz/jes7YeVjg3FpcoI881H+fbKXTqX1/BiWCSX/5+xouHDhAvr164chQ4Zg0aJFaNCgAfbt24c6deoE+qmIiIiCwlOywi3z4SWxoTa75an521T3dQYy3ZvXgaE8VSEGH6bybdd2TcH24zlIjI3Cuxn73B7HYg6P1Wu1BDz4eP3115GamorZs2e7tqWnp3u4BxERUXiRtVB3u63i+5V7z6BuLe0sw4crD+DGHvpbmjsfWxxqkQ27GCsCkuevaY/d2bkawYf7wIa/a8QEQ8CHXX788Uf06NEDN9xwAxo2bIiuXbvik08+0dy/pKQEubm5si8iIqKqpFw8rsxmxw+bj+PYhULZ1NuXf97lMfPx2qLdeHvpXh+e1/FgYnNSsbzDqOhaatQo5IiJCu/MR8CDj4MHD+KDDz5A69atsXjxYtxzzz144IEH8Pnnn6vuP336dCQmJrq+UlPDY9EbIiKqueR1HhLmZh7F1K+3YOAby1X6fHged1l78JwPz+v4v7zIVP17x37qj6Oa+QifxEfggw+73Y5u3brh1VdfRdeuXTFp0iTcdddd+PDDD1X3nzZtGnJyclxfWVlZgT4kIiIin4gBhc0OrN53tny7SgGql8ey6pxm63gslfbqRvdhFyeDEFGIwYXFh9Vvq0LAj65Jkya49FL53OJLLrkER4+qV/ZaLBYkJCTIvoiIiKqSfEaL9mwXxwbPj6W3wZjjuRz/N6oUmQLuwyziT1HC+ExMmBecBjz46NevH/bs2SPbtnfvXjRr1izQT0VERBQU7lNt5TNctPZV41PmQ6XmQ575kO8vBiNRwn5qmY8wGnUJfPDx0EMPYe3atXj11Vexf/9+fPnll/j4448xZcqUQD8VERGRV2U2O4pKbT7dR6zrsNkljx1PAznsolrzodLnw/WzGHwIdR7hPtU24MHHZZddhvnz52PevHno0KEDXnrpJbzzzjsYP358oJ+KiIjIq6tm/Y7e0zNQWFqm+z5iV1KbXV6J4WvBqW/DLo59xRDDrDEEA8jrPMxGYdglzGs+At7nAwCuuuoqXHXVVcF4aCIiIt2sNjt2Z+cBAHaeyEWP5nV13U+Z6ZDchmEqlJZ5zmzoXddFfF6Dlz4fTmLwEWUShl3UMh9hNN0lvEMjIiKiSjidV+L6Pj4mSvf9ZLNdJEne90Oxb5HV85COL8MuTvI+H9oFp+LPZlnwEd6n9/A+OiIioko4lVvs+r7M7kvthfawi7LgtNjq+XGdu0d7WQxOJOtw6iH4EH9Mqxvn+l4t2xI+eQ8GH0REFMFO5QjBhw+1F/IaD8+zW0q8ZD6cxMyEN2JQIWsy5mG2S+uG8UivXwsA0Llpku7nqgpBqfkgIiIKBxcKra7vfcl8yGo87JJ8rRdFDFOsN/jQakeqwqARcChnu8gai5mN+On+/jifX4q0enEIZww+iIgoYolZCl8yH7Kptm6zW+Q/e6v5cIryadhF/N7DbBdhMCXKZERtixm1Leqn9jCqN+WwCxERRS5Z8KGytL2e+9nd+nz4VvPh5Muwi9YMF+VsF/FHXx6/qjH4ICKiiCX26/Cp2Zewq2O2izDsothXb+ZDmbXwxODPbBcfhnWqGoMPIiKKWMpOpfrvp5jtIq71ongcvUGNsl7D475+9PkwexnWMYTRfBcGH0REFLHEIMKXTqPKYRZxsq0yhtFbS6LMWniimfnwsKotMx9ERERhQF7z4W+fD/ltymXv9WY+lFkLvTwVnMpqPrw8PgtOiYiIQiAgwy6S54Xl9Bay+hJ7iA3JTB5WtZVlPnyYTVPVqs+REhERaZAkCQu2HMf+0/my7f4Ou9gVNR6emo6VBTjz8diIttp9PiqR+Qgn7PNBRETV3pKdp/DgV1sAAIdfG+3aLsYJeoMEx/0UC8l5yHzoDWr01nx4mtHiPtVW+zalcApNmPkgIqJqb0vWRdXt4lCLb30+hMdQTLV1y3zorCXRm/lQ7uapz4fIlyZmVa36HCkREZEGZTbCSQwUcoqs6jt5uZ/7sIt8X/01H/5lPkwa026VP3trMsaCUyIiohAQ44I3F+/Bgi3Hvd7HarO7zXaRdzVV1nzoDD50Zj6UQYJ4v1oWk/y2alrzweCDiIiqPeX0V9d2RUrk0W//8vg4//n9ENo/txhrD553bXNb1VYxyqK74FSIDWKjTJr7uWU+hKCicUKs7DZ5n4/qc0pnwSkREVV/GskH5fRab10+X1q4EwAwM2Ofa5tdkoc2ykBHd3t1nbUbypvE36FxYozmvt6HXcInM1J9wiQiIiIfuZVj+HH+dWuvrnjMC4X6akn0zkxR3na+oNT1fZ24KNlt1TXzUX2OlIiIyEfKYRd/yiKUmQ/lMIxeeoMPZYbikiYJmreJ/O2gWhU47EJERBFLGSj4s7iaI/MhrGrrX+zhw7CL/Lb0+rXwywMDUD8+2uPjR3kZdgknDD6IiKja04oHlEMk3pIDMVFGFFvlBaR2CYphFz8zH0ZxiER/zQcAXJqc4L5RoXn9Wn4dV1Vg8EFERBHLreDUS9GlxWxyDz7s8lVtfVkjRpRXXFEb4kvmw5vVjw9BYakN9WtbPO4XRvWmDD6IiChyKWs+vJ1/1YKCxTuyUVBaMaPF38zH5qMXPT6Pk69BQmrdOL+Opyqx4JSIiKo9ZZDhpExSeDuxq2UdxMAD8D/zIQpk5qM6YvBBRETVnp726oD3YRc9y6P4sD6dJk81H8GatdIxJTEoj+sPDrsQEVHEcg8+Kr6XJMktGDHpyDr4O+wiex4PPTkCnfhY+tBA7D2VjwGtGwT2gSuBmQ8iIqqWcoutmLF0L/afztfcR9kK3TmksWrvGVz2ym9YtvuU7HY9XUADM+yifVugh11aN4rH6E5NAvqYlcXgg4iIqqUXftyJmRn7MPztlZr7uPf5cLj1s3U4m1+KO+ZswJm8EtfteoY8ApH58BRgsOaDiIgoTG084lj8zS7p7/Ohltl47LuKxeb0BB/OVWyVD9Uw3vNU15fHdnB97zn48HoI1R6DDyIiqpb0jH4osxRqJ/btx3Nc3+tJOmgNu3RqmuTxfuN7pXk8jopjiPzog8EHERFVS3qGPzwVnFZsq9ho1xHRWJWFJOXiok0e7yc+DzMfRERE1ZAYV8janwsBhDKWUIstxJN9mY7gwznsohQb5Tn4AIBHh7dBUlwUnhp9ieY+1WmBOH9xqi0REVVLYlZDbH9ulyQYy0tLlZmPmCj3a24DDCgqtaGgtExf5qO80YcB8lqTWC+ZDwC47/LWuHdwKxgMQI9mdbDzZC4KFU3MakLBKYMPIiKqlrSGXewqWZBhlzTEb7tOqzYjMxiA/q8vw7mCUl0rw2plR2J0ZD6AigXmvp3cBydyitHvtWVuxxPpOOxCRETVklaSQgxKnN83KJ+JojZkYrNLOFdQCgCwagypiMqcmQ9FlOCt5kPJYDAgIcY9B1ATMh8MPoiIqFrSXs/Fsd1ml1wBisXsCAzKVIpFTwt9PvTQClDMOrImSrWia2bwwWEXIiKqluwaBac2u4Qv1hzGa4t2u4ZCLGbHtXZpWeUXZnFOtVWGCIUlNvedvTCqFJfWgHpTBh9ERFQ9ear5eHbBDgBwFXNGlfczzy0uw1uL91TqebVqPvJLyir1uE7s80FERBSmtGamqA3HRAmLqby3fH+lnldt6AbwveZDS02Yasvgg4iIqiWtHmNqHUj9qcfQkltkBQAMbttQtv2uAS3Qr1U9/OuGzj49XtM6sbKfa0DsweCDiIiqJzHEkM9wcd832tMysj5yPv7kQS1k2+vUisbcf/TG9d2but0nrW6c5uMtenAAPp7Q3fUzh12IiIjClNqUWuX3Tnr6d/jKYjapNi1TM2lgC83b4mOiZOvC1ITMBwtOiYioWpEkCf/NPCrrDGoTyjBUgw9z4K+1jTof8u2/d8bYLim6H4tTbYmIiMJMxq7TeOaH7bJtntZzAYAovZGCD/QECZc2ScC1Xd2HYZRMwmPVgNiDwy5ERFS97DmV57bNJg67qEQfUebAn9E9zUqZf29fXHFpI3xwSzddjyUGMjoW6632gh58vPbaazAYDJg6dWqwn4qIiGoAtRbp3ms+gpP5MLi1GnPomlYHn9zaA83q1dL3WDWh0EMQ1OBj/fr1+Oijj9CpU6dgPg0REUWwjUfO49iFQtfPan02xGyH2lTbYAQfgezHIT5WDUh8BC/4yM/Px/jx4/HJJ5+gTp06wXoaIiKKYDtP5OL6D9ag/+vLXdvUOoyKyRC1tVfMPgYK/72zF67unOxxH1MAizNMsmGXyA8/ghZ8TJkyBaNHj8awYcOC9RRERBThNh4577atzOY581FkdV9jxZdhDaMB6N+6Ph64vJXn/QJ4BhUfK/JDjyDNdvnqq6+wadMmrF+/3uu+JSUlKCmpWFEwNzc3GIdERETVUInKQnBqmQ+xzqOw1H2NFV+mrzqHQLwFLCajIWAzU0wsOK2crKwsPPjgg5g7dy5iYmK87j99+nQkJia6vlJTUwN9SEREVE2pBR9qNR3itmK1zIeXICE5seJ85eyG6m1YJaDDLrIDjPzoI+DBx8aNG3H69Gl069YNZrMZZrMZK1euxMyZM2E2m2Gzyd8U06ZNQ05OjusrKysr0IdERERhwGqzY8bSvcg8eE73fUpVgg+1mg4x81Fidb+Pt0Cha1pFbWJ0eUMyb9mSQM5QqQkt1UUBH3YZOnQotm3bJts2ceJEtGvXDk888QRMJvmqfxaLBRaLJdCHQUREYWbWsv2YmbEPMzP24fBro3Xdp1SlvsOmMttFzHyoZUu0Tu71a1vQISUBiXFRrm2u4MPL5XkgMx81TcCDj/j4eHTo0EG2rVatWqhXr57bdiIiqjm+zDzq833UMh9qfT7ETWr30UpS/Pnk5YgyGfD8jztc2yxmx0Wyt6m0RqMBA1s3wK87spGSFOtxX180SQzcY4UrtlcnIqKQOF9Q4n0nBdXgQ1HzEW0y4mJhqevnkjL3mg+zxsJyziyHSUhzuLZ5q/kwGvD69Z3QKTURY7ys3aLHwvv7I6fIiuQABjLhKiTBx4oVK0LxNEREVA39tvMUtmRdxMNXtHGroxADCbtdgtFocGsyZoky4kxeiXAf94AlWjHkrySuemspDz681WEYDUDtuCjcO9jzlFy9OqQkBuRxqgOu7UJERCGhdTL/xxcb8N7y/ViyM9vtNjHz8ecBR6GqctjFZpdwNt9L8OFlVVtxiKUiGyI/3s5NEzGmS0XjsZqw+mywMPggIqoB7HYJMzP24fd9Z6vsGLydqrNzit22FZZWZD5u+U8mAPeptoWlNtkMGH+CD7PQft3TVFtxWyDbq9c0DD6IiGqAX7afxIyle10n8GBbtO0kbvjwT5y4WKT7PsqTeWmZHatVgiWrSp8PkVrNh9fgQ3huS5RjiMZtJVyDAbVjKqoVONvFfww+iIhqgCPnCr3vFED3zN2E9Ycv4IWfdnjfuZyy3uNsfolbq3RJklSn2orU+nxEqyws99Wk3q7vo1QyH3HRZqTWrSj+NAC4e1BL1K0VjbaN4mvcSrSBxOCDiKgG8HWxsqJSm2wGib9yiqw+PafYGt2q0uMjO7cYuUXu7dOVj6OkzHw0rROL3i3quX5ulFDRb8oSVbHvkqmDXN8bDEBKUixWPT4EP93f3+MxkGcMPoiIagBf1wsZNmMlury4FBcKKheArD143hXEiKMUzoXgxKDo5Z93ofMLS1w1HWrTbPtMX4Ztx3M8Pmd+iXtwosx8RJmUwUic63uLcFtsdMUsGefh17aYvQ7jkGd89YiIagAvZRJujpfXaqz1oRW6llnL9mP78RxZUai1fOhE2cHUapNcwYpa4ageeWrBh1kZfMiHTMThFa1C0k5Nk/w6HnLHJmNERDWA5OdiZRd9GDZx2nVSvjp5dm4xrpr1u2yb1SbBYlZfpyWvuAz1altUW6vrkV/sfszKgMKs6J3eML5iYbnsXPmsm8VTB+KXbScxaWALv46H3DH4ICKqAXwZdrELaZKLhd6DD5tdgtHg6ONRWFqGUe+ult2eV+yeiSgqtcFiNmJPdp7bbRcKS9EctWD1N/Oh8nxKysyHGJwoj6lt43i0bRzv17GQOgYfREQ1gBh7/LD5OM7ml+AfA9Sv5K3CbBJvRaelZXaMfHcVUuvE4fM7eqoWmOaqbLvsld+QkhTrGt4RObMtfmc+VIZdlMwqs196ptfFukPnMaJ9Y7+el/Rj8EFV5mx+CT7/8zBu7JGK1Lpx3u9ARP4TUh9Tv94CALji0kZoVq+WsIsEg8Eg6yCqzHzsPJGL/2YewdShrdEwIQZbj13EwTMFOHimAIB7AzAAyFUZBgGgGng4ntMR8KgVnOqRryPzYVap6/hkQg8s3pmNUR0YfAQbC04VcgqtWL7nNMr8jLhJvwe/2oxZy/Zj3Cdrq/pQiCKe2qiL2D3UarPjqlm/4x+fr5dNcS2zS3h76V4M/dcKnMsvwZUzV+PLzKN47Lutbo9nt0uqRaLOwESvi4VW5BRa8daSvT7dz0mt4FRJbaG5xLgo3NgjFfExUX49L+nH4ENh3CdrMXH2esz+43BVH0rE+2O/o4r+2AX9HRCDpcxmx3vL9mHT0QtVfShEQWFXKfoQZ4BsO56DHSdy8duu07h/3mbX9uMXC/Fuxj4cOFOAVfvOuLbvzpYXlQKOYZJiq3uPDV9dKLTinwu2uxWuBpJyqi2FFl99hZ3lb/aftp6o4iOhUJq3PgtvLdmL6/79Z1UfClWhAh1XzNWVWsGp2GNDHGoRW5rvP53v+v63Xadd36stquYIPiqfNc4pLMXvQqATDMrZLhRafPU1xER5Xn6ZIsselau4SLYnOw+zMvapdoL01+ncYrz6yy4cOedbij1czFi6F+2fW4zle0573zmEiq02n7uTqlF7hDKhPiO/RL0uQxya+XnrSdf3zuBDfNzSMrvquiq+ulBoVa0dqaynr7zE9b1ytguFFoMPDXHRDD4oco14ZxX+tXQv3vnNvzF1NffN24yPVx3EDR+uCdhjhtLMjH0AgOd/1L8WSWWt3ncGT83fJmspLsoptKL39AzcNnt9pZ9LbdjFU2GpU6FGgOpMHIjTYa02u+q6Kr66WBSY4OOmy1LRp0U9vH59RwDAXUKfDrXZLhQ6fPU1BCv42H86D5//eVh1zYLqrsxmD8hV7+p9Z3Djh2tk6d5gM3hd7DsyrTt8PmCPtb78sU7nlQTsMauC2kk6WCb8Zx2+zDyKj1cdVL39r2MXcbHQilV7z2jOGNFN5dey+djPQ+Rc0VWcDltaFpiaj5zCUllWxl8N4i2YN6k3/n5ZmtttUVwUrkox+BCIqU3lsIvNLmHRtpM4peh8p+VkThE+XX0QeYoPjGEzVuG5H3fg/9Yc8fs4953Kw/7T7o15AuWnv05g5DurcPCMbyf/R779C4PeXIEf/6pcvcyE/6zDusPncd+Xmyr1OOTdnuw8/LHffclyf0TKR7mXBVP9duRcgeYia1rLzluEgtC/si5W6vlVMx8+9PNQcg67iNNhS8vsKA7AsMtfx3L8aq3etE6s7OfEWO1ZK1ot1Ck0GHwIxKWbYxXBx5frjuKeuZsw+M0VWL7ntNfo/s45G/Dyz7vw7AL1FO6GI+f9yn4UldpwxdurMGzGKr/nwHtz/7zN2J2dpzqVzpMFWxxBhzN9XVlnqvkVdCgUlpaVr5lh9+vKuLDUhvGfZgYky6RWgFgdBaK+QinrfCEGvbkCg95crnq7SaP4UTwBn8vXDg4kScLSnadw9Fyhh33ct4nDLrk6emOIDAZg+/Ec/Lo927Vt0fZsHPFwDMHQo1kd1/fJibHonJrk+jnBQ/DBjqVVi8FHuRlL9uCmjyv6TSgb0CzbdQqAI0CZOHs9np6/3ePjOWfN/LDluOrty3efQftnF+PbDVk+HecF4eokEFcYnmTn6MvyKBVWYsZAOMw2CEahW7Dc9tk6XDXrd7R+ehE6Pb8EZ/P9C9iOXfD9hKEsLgy32MNqs2Ph1hM4ned4H+cUWTHpiw1YtO2k2745wpCDpz//vHVHsUDj37SWjUcuYPqiXQC0hzbUGl4BkF3keOr2uWLvGdz1xQYM1AhuAKgOY4jvdV8vhowGA66a9Tu+31zxesxYuhfv/BaYiw+9UoRsh9VuR4yQLUpQ6dfx3eQ+uP/yVritb/NQHB5pYPBRbuay/dh6rGKZZquXE9D/Nh3T9bhaF1FFVhtKbXafswvih4U9ACfJn/46gaH/WoGdJ9xne2gVwXlTIBSoLdp2EuM/XYvT5cNVno75931n0f65xV4f/41fd2PKl5sC8vs7iSfOylTr7z+d7/pdfTFv3VH0f32ZzxmI9YflfUlWa0xPPJ1bjH/+sE1ziMVi9q3GSZIkXDVrNfq9ttz1ehk0oo+iUptqpjCnyIrNRy8EJdMAAJ+uPoT7vtyMMe/9AcCRkVuy8xTumSsfzpMkCZ1fXOL6Wavm43RuMaZ9vw0PfrVFd4AqSRKu/+BP/LKtIjtQZrNDkiRZwKc1BCBmPspUFmBzyjzovXanTGU8aVd2Hj5YcQAlZTafgw9bCGtjPBEzbmU2STZknhDr3sS7R/O6eGR4W/b5qGI17tXPOl+oa3qhckEjrQ9WwPEh+uNfJ7yerANxRS1evfi77oHo/nmbceBMAd5cvBuHzhbgH59vcN2mVeXujfj63jN3E/7Yfw5Pzd+GX7efROcXlmDZ7lOuq1HRP3/YJvvZ+ZtabXZsyboIm11CfkkZ/r3iAH7eehIHPNSkFJaW4ZsNWTinMxMgfo6+uXgPZiz1bRbIhsPnMeePQxg2YyV6vprh030BYNr323DsQhGenr/N+87l1GoAtD5Qx3+aif+uPYq3NX4vb8OIp3OLMWPJHpzMKSrf3469p/JxNr8E+045/g5q/0KKrTZ0eXEJhs1Y6RZkXPPe77j2339i2e7gTG1dstNxwj9ZnsHTGsZTrgOi9a9U7Jqpd8hTbcVW53u4/+sVWQqtzEeJYiaJFj0BnFrw8tLCnXj9192YlbFf9Vg9uVDgW41IIPRKr+u2TXzlrDY7YqIq/g14qvmgqlWj1nbZeuwirnnvDwxq0wCf39HTtV3tH64vVwH3z9uMVXvP4IbuTdGsXhzWHDwnu925XoKy+BQAUpIqUoaLtp3ExSIrxvV0r8x2Eq/KPV0J6SFmDmKjTfjXkj34rXx4yfFc3l+D/JIy1Io2yYIztaBoS9ZFV4OiO+ZsUE3Ra/VWeXbBDsxbdxSTB7XEkLYNKo7fw6//wo878fWGLHRqmogf7+vv2u78WyiJf29nd9vb+zZH3VrR2k8i+FuAppcW6zypFVttGPP+H27blcHHr9uzYZck7CvPqGw4ot7BtchL8HHP3E3YeOQClu46jUUPDpDt71xBVO1vujs7DyVldhy7UIQyuyTrreCsDfhlWzaGXtLI4/Nr+W3nKZhNBgxu29DtNuXhKM/vFwpKMeb9P2Q1A4D2iVy8wi4tsyNWx4w4tdc1r7gMby7eI9tm0ug5IQaFHoMPxc9n80swY+lemI0GPHXlJYiJMnkMLpbvOY3m9Wtp3q7mgo+zY5wsZqPPxaTN68WhyGrDm3/r7D60JLx0ZXZF5oNt0sNWjQo+nFezK/fKU9NqY6HKf6iehrNXlT/etxvVh2JyiqxIiotGbpF7ZkSMzJ3p4D4t6ml+EIhz6P0NPjYcPo+0evKF3FLrxPlcKLb/dD6GzViJsV2S8c5NXT3ue1ZRLKf2+W5RCT5O5xVj3rqjAIAPVx5Aat2KYM3T8IizQ604lPb7vrN48KvNePW6jm6rVqp9GKqlqdWonay0ghwdD6ZrN7VVQgHH+9Rul2A0GlBstWHyfze631clCNbKBi7cegJtGsVjY3nQ4mx3LZ5UnXUmagWn4uyO0jK7amZGedVfbLXpavKXU2jFP75wZOr2vjwKOUVWzFq2D+N6puGSJglu+4t/j4uFpbjugz9x9Hwhjp6Xv+/VgtonvtsqG2otsdkAqJ/Y8kvKcNtn69CucTweGNra/bhV/nb6Mh8VBzY38wj+vfwAXhrbHpe3ayS7kFi2+xRmLN2L7ccdf6vSMjteu76Tx/dzTpHVlc2JNhsDUswebTKqXoj0aVkPK/b41r30keFtcVWnJqr/psRp8mU2u+zvV0fnxQOFXo0adnGmhwFHFmTNAUeGQu0fmvIqozLFdM6VG9U+dJyPK354nPEwVCC7EvJjTuC6Q+fxtw/XoP9ry1FYImRR7JKscMtpyY5st21O//n9EADghy2Vb0X/zYYst2EESZIw/O1Vsm1i62ZPV09qf64Jn2XiXEEp7v4/9xOy6rCDzthO7QPWlyu7QPRFcPq/tUfQ+cUl2HjkguYxdHp+ids2ZfFySZkNM5buxX1fbnb7GwDyYMXZ10PtNRenb2qd0MSr/p0nctHumV/x4k87VfeVPXaR8Ng2Ox7/7i98seYIRr272m0o5ae/TmC+UBj5j8834NBZ9Z40ypqP4xeL8PWGLNlFiqdA/f3l+7HxyAXMzTyqGtRtO57jtk0riycG2OJn0tPzt+P4xSLcMWcDDp7Jl93/jjkbXIEHUFH07uliJafI6nr8egE6YcfHqF/bPjmqHR4Y2hq/Th3g02NpBfNi3Ga1SbLP2dqWGnV9Xa3UqOBDvEK95r0/MO6TtcgptGoGH2U2O278cA1u/GhNpRreHC9fOE1rKuTyPadl9Que0queCtAkSZJV7atxZmlKbXZZCl6r4GySyom6tMyOzIPn3PYXi+acqwK3bljb4/E4Pa5SeCvBfXaA+LfS6qSYX1Km+kHlKamgFgDoralRO8E4/05n8krw1bqjmlkam13CyHcqTu5632Vax7Z631nkFZfhgXmbsfeU914wzmmJRaXyFt5Pz9/uccq0+DufU8l89H41AwfP5ON8gTxAANwzRWajAX8eOIsbPvzTlan57I9DHusYJEmSnXDLyuuCnLq9uFT2HhAXSgO0h58A96LoDSqN2G74cI3r35LSDqF4W5wJ4jTte/e6ng9WHMCXmUfdtovB9u7sXEz4T6bb8ew9lY+txy6qHgvgCGxm/3EIP6vM8nHKLymrCD5qBzf4iI+JwsNXtEG7xu7ZKS1x0dpBhEE27GLX7KVC4aVGBR9qLhSWqn6QL99zBq2eXoR1h89j3aHzPqcJRc4PYLVU+Y4TuZg4ez2uEK4ulUM+v24/6crSeCpAe2r+NtdVr5MkScjYdcrVHE2sUH/0279c3xeV2rHukL5uly/8tAN//3gtvlMMM4kfNs5hlsoU2apNSxSDD7WA4cCZfHR4brF7EaHkHqidyi3G9uM5OJ1brD7sIvwdbHZJ83dRG9d/4acdyC8pw2uLduPJ77dhytzNKvd0XHEeFq6i9U4g8FYcaJckXW3OnVe5L/+8C+nTfsHm8lV9lX9bJfF3/veKA45ZNsJJIDu3GF+tz5JN/R385gp8sz4LXV9aKnt8k9GAmz/JxPrDF2RDINe894criBW9vXQvLnslA4eFzEWpze5WjO3vey+3uAxFpTbsPZWH3dm5sgBK9N6y/bKfJUnCgTP5svelLz1vnlIUG9vtEoqEIvZftmVj9b6zbvVFz/+4w2MwVVpmxwteMkkmgwHWMsfrVbeWRfcxe9IrvZ7r+8vbVdTk+NNZVGy2pjRJaJleZpM0hyQpvNT4nFRBaVnQO905T4R6m0D9uv0k5q49gunXdcTna464PsAOvHql7IPtVG4xXv55J8b1TMOYLimYt87RM2Rmxj5XQe0PW47joa//QpPEGKyZNlRzGqH+qcMS5qpcodntkuxkfTqvGI0TYwIyI0ckBlxqAcOnq93bVA94Yxl6Nq8n29b3tWWuWRDenstqs2PkO6sQF23Gj/f1AwAcOluA5vVqwWg0qM4K+n7TcdjskqvxmljICzgCp5d/3olOTZNk2yVF7uP95fvx6/Zs/PcfvWT1Qd4Kor39bk4JiqvTZxfsQHdFAaYa5cyuCf9Z5zazIDE2StYRuMhqw+P/c2S4xMBXa4bOtuM5OHi2AM3qxcmmAr9b/u9h4pyK9U6sNvfgsDLDWW//ttfV8nzyoJaq+6w7fB6/bj+JkR2aAHA02Zv69Ra/nxNw1M/Ur+04+d/40RqPQYVTth9Tu5XK7JKr1X79AA27XNctBd2b1UH7lATMyqgI1PxZU0WrWdiuF0ciNtqEAa3rY/W+sxjXMw17T+VpDqlR+KhRwYfahVBhqc3nHgdOO07k4FKVwjalghJHSvuJ/+mbRukMIixRJvwktCo/dqEQC7dW/Hxn+bTYtQfPY0yXFNf2lXvPYMbSvXhoWGs8+4Ojw+rJnGKsO3Re95W1qMxmh9lkxKu/7ML3m9QbLBVabbKsxEsLd+Kbu/sEvAvrr0INijiU8fmfh/HNhixZytsp63wRss7Lgys9J+fzBaXYccLRwfHAGceHWZHVhgVbTmDa99vwwOWt8PDwtprFmnuy81C/drRbsS0AfLzqIP679igAeSCn/Ps4Z0V89vshPHRFGwDAN+uz8Nkfh7wevx7KgMFmlzDnz8Me71Nstan+zsqRLuWMDi2e+rW88vMu/LH/LBY9OACtGtbWHPe3lrlnOirz3hOHQLLOa9d3TP7vJhx+bTQAR6BYWQ99vQX/d2cv2OySrsAjGHq1qKs6XOSrxLgo3NgiFQAQJWQufL3Yu2dwS6RrFOA7Zxx9eEt3bDxyAX1a1sOFglI0iLfglt7N/DxyCoUaFXyodQTNLynzuyhp9Mzf8c/Rl3jdr7C0zK2iXo+fFGukHDiTj8U7Tqnue9krv8l+npmxDylJMbLeBDd+tAZ39k/3+TguFllRv7ZFc/ErwDGkJGY51h++gB//OhHwRcbEJlxn8kpQUmbDxiMX8FwQViL9u9Dx1qmw1OYas5+5bL8j+NC4wjYaDJqBiVYzMWfw8dqi3Zi7tmL9H2e3WbtdcmUPAiFeMRVRzxTzbi8tVc32+DvM8env2oGUc2baSz/vwtbyRdbUWFWGWfxZG8RJHLY7prHuitPFwlIkxUUHJMu3ep+jCVxVLjyZWjcO79/cDVuPXcTVnZNx1azf/Xoc8XNVHGrxZSn7Cb2b4YmR7bzuV8tixsA2jmn4DRNi8Mq1HX04UqoKNSr4UPvALCyxVeoK6eWfd2neFhNlRLHVjvySMhSUVH5Gw4HT2qlEtQZKC7e6F5j5s2LnqdxibD560eM+81WulB78aovPz+WL6Yt2Y/qi3UF9DqVCxd/xv2uPaAZYxVabLDBxToEtLbNj+wn3GQ+Aoy3/W4v34MOVB2TbzxWUosxmx/hPMyv5G8gpiwKVtTJqtJrP5fm4NogvtIo7nXaezHXruOmtd4lezunFWt5fvh/9WtV3a0yo5oHLW2HmMs8ZkgFvLMOkgepDPaEQbTJidKcmGN2pie4mfWrEIlEx22EW1rFJiovCxUIrJg1sgV7pdTHt+22yf0/h1rKfAqfGBB82u6QaZBSUlAW8LsGpTlw0TuYUI7+kzLXceGX4mj0RpxY7+XN1Onqm9ysfvSn26q5AUevwzx+01/g5qBh3LrXZEWM04Yn/bcXBM9qB5Hsq6fvfdp3Cb7tOI1NnUbAe0SYj4hSNsvTWioQbtUC3JEDBh7eLk09WH8Inqw/pmqLaIN57MWfW+SI84+F9FWxiDU5tjRkreoiLc4qLuIk9TRbe3x/Ldp/GjT1SERNlwrqnG6H5kz+7bmfsEblqzGwXrdbnj/9vq8eK9KQ4/zvkJcU5PowWbDkRkGGBLB8X/1IrRPO3ZTo5VKaltHNqsFqWSI9ZywK7YFeUyaCrmVd1VZlhF3+c0/HecBaThjMx+PC3Hs5x34rHufGyVKTWjUWX1CQYheCjaZ043Nqnuex9OHviZX4/J1UfNSb48LSei3OcValBvAWrHx+CbmlJAIB2Pi7BnKRSod2igW8tjEWVme7r5G0KZbAor7Crq5srMexR2f4DasW0lWGJMkV08FGZ3jxKXcs/AypLzHy8NLYDRndsEpDHBXz/fNISbfYt33BzrzT8rXtTt+1ikJEQE4XfHh6E+ff29fp4Q4RW+X51CaZqocYEH/5c8ReV2hAfE4U5d/TEvLt6464BLbzfSaB2svHWBCwcfPmPXgF/zFAt8DTvrt4heR5/DHxzOQZ5WPI81FKSYmWpcdKmZ1ablp7NKxZDEzMfKUkxqidtfz05Sl6Yef/lrfx6HF9Xe3312o4Y1zPV634Ws8nnYEJtd+drdu/gqquLocqrMcGH0WDAZc3roKGOMVen/q3qA3BE7X1a1vN5CGZUh8Zu2/T2+qgMsaFP56aJPt8/GEtl613gqalKi3e9hrRtgJ4qq15Wpeu6psh+9nX9HCW9C93pYTEbPS6OZjIa0MLHxcYiVZtG/mcVuqQl4eZeaZg8qKUsCDcbjWiYELhhGOXMpUeGt/XrcfQEHwNaOz4br7jUsSBg92Z18dntPQI+rGRQqfqYfl1HzL+3r9+/H4WHGhN8pNWLw7eT+2LlY0Nc22bc2Fl135goI67rloLXr+8k267V6AYAxnRJdtvWulE8fnlgABoJHzD1AtQ90Ol/98jTmClJsWicGOP6eXh79wDIqVa0CXcNSHdb1Koy6eolDw1U3a63cK0ywwAJsVEBbxintz28lloBWFuijhD0/jp1gK7p3Xq0blTb4+v94339EO2hs6Q3HVMS8Z/bevh9/3CSVjfO+04aDAZHduDJUe1k7webJKFRQoyHe/omUOuY6ClKf/aqS/H0lZdg1riKBSUvb9cIzer5/zqpUct8RJmM6JpWJ+jNISm4akzw4SRe6WldzYxs3xgzbuyCxDj3jo1Os2+XF0Wp9c+wmI24NDkBKx8bgo8mdEfn1CR8cEs3nz/IejSrg14aV/Rdy9fmcIqLNqGW8Ds2r6d+5dqifi1s+OcVeHr0pUhVHE+Mj0VmzmXu0+vXQptG8Vj9+BC3fcSaj3s8pEs9tVH2JhjLZ2t9DA8u/52d+rWq6KC6YEo/1/eBCD6eu7o9AODugS3QMD4GI1Uyar5KrRuLR4e3RUyU+uv94NDWaJ+ciJYN/Au+6taKxtd398bQSwJ/QtLy3zv1DRc6r9q9ETOXlQmKxUSiGMyV2STUjfOeyVJbGVeN+Ld0npiHl2cmtCg73Na2mDUDouTyi5p7B7dE60bxuGtgC7fXpTLLKahp06hywT+FrxoXfADAbw8PxJyJl6FDivqQhFb7XzH4ELMg0SYjOjVNws8P9Jft70xfxkSZMKJ9YyyY0g9d0+rg3+O7aS66pEYC8MWdPVVvMxoNsg/dWhazbH69uAR9++SKcevP7+jpCsSSkyo+bPq2rIfeLXwbunjzhs64//JW+KK8pbvadELxQ+rGHqk4NP1KzFGpaq/Mh3ycxb/71hcW0lIGFVoaKz6g68RFo0NKAtonJ8jeV7V9PCbna+j04S3dMLZrClY/PsTVbCk50f+hKafXr+uEerUtmjUfzvfGG3/rpHr7P0dfgj4t6qneBjhOas73oadU/GXNvbdy16tfq3qyv6UWrVHFpLgo1zBTl9Qk2XLs4oldT/3Szb3ShOdTf8K2jeJlRZnDLmmo2oBLq939SEVWU5yZ4nycD27pjm3PD5ft56zPGNquIb4TMqe39E7D+qeHuWW7Ft7fH3cNSMevDw3EX88Ox+Memn7500dIzfx7++KJke3wt+7ea0moeqqRwUerhvEYLFRUK5k0iqLEDx3xCv2BoY7CrvbJiVjx6GDXdrNGJ78OKYn469nhuHugvgLWO/qlw2I24d/ju2HSwBaIV1xNi1cHcdEm2YdVWt04vHdzV9zet7msYFbMEojp2vdu7uZzUVj92hY8MrytK4OiFkCIJ7kokwEGgwH9WrlfgbZuWBspSRUn19v66G+RXNvDypdOyiGLFg1q4R/C6/L0lfLbxRPHN3f3cX2v/IA2GAxYMKU/Ft7fX5YOFhfp6tQ0URZ0qmV5nF0aHY8J17ohqXXjXCcqo850c7SHsXtnu2utmo+Y8ttrWcxY+tBAXKIouBzXMw1PXak9/CMufPfo8LYY0Lo+miS6X1G/f3M3/P6Ee6bME60A0WAwIOORwVj31FAsenAAru6cjJfHdnA7duX6OYAja7fl2eFY9uhgrH58CL6+u7fs35l4Ys8psuLrSb1x/+Wt0DDegpSkWGQ+NRSthCE6MaOhTAasfnwIFkzph7TyjNCnt/bAvYNb4uMJPbD9hRFuxzawdX3MmXgZPr21Ygjru8l90EUxA0d8TzpXwzUZDW61INOv64RD06/Ef26/TJb9Tasbp/p+6JCSiKdHX4qEmCi3bLBSoDIfXdPq4J7BLTm0EsFqZPDhjVbTsZgoxwJGHVMSZU1zxGC/jvChY/RwEjcaDbIrKy3fTu6D0Z0cJ6ArOzbBU1deImuZDkD2gWCzSxjYpgFmjuuKWeO6IikuGld1Ssbz17TXbB4kdhz0ZfbDfUNayU7IWib2ay57XOdJMcpkxMtjO8j2nTSwBVY+Ntj1s9ZHWYcU99kHWkMcrRvWxl0D0tG2UTz+flmq221izYsycBKn/Ynn8iiTEd8L0wYlSYLJaHAFbvdf3go9mtXBdd0qCk4tZqOsbbb4XqltMeP6bvKZD5W9iLzDQyt953tT/LuItUligNO6UTwWPThAFgjGRZs8nhjEf0N9WtbD/93ZS3V9DkuUCU3rxOF/96i/j9Se4/a+zd22Of+GibFRaJgQg0uaJGDWuK64pXczTBlSMcyXGBslWyju/stboUNKAn64t2KoLLWuYyE7MSgXMx+xUSb0alEPjwxvi98eGYRfpw5Ao4QY16KDgLxzrPLvmFo3Dp2F4dJhlzbC4yPbwWg0qPbVMBgMGNy2oWx4NNpsdAtefanPES8w3vl7F4zu1AS39mmu+/5a1AJMIjUMPlRoLaENONLiP97XT3YiFz9bxGyHtwRCHZWriGb14vDt5IoP4suauw+BOKfQPTbCUe0tfmA5Gytd0zkZV3eWF8GKzyd+qIvfKz/QOqYkqp40UpJi8eiItrpmlzx3dXvZyUx87cQP1J8f6I8WDWrLhr2sNsnVdKhFg1r4eEJ33D2wBX6c0h9/PStPJzuvBK/v1hQWsxGf3d4DY7ok45Nbe+Dp0Zdi8UMD3a4CHx3eVt76WZGtEivqo0xG3NijKeKiTZjYrzm6pVVkmJRxwiPD2+K7e/rKgplOTZNcV6QAcPcgR8ZlYJsG2P7CCPxLowDaH5/c2gOPDG+DW71kjmTNnW6vGPIRMxdO4t/KYDB4TLGrrU3y1JWXINpklBXxOt9vajMsJg9qie8muwclajN+tLKMgHyIbNMzV8hqrqYMaYWF9w9QvaIXA4iYKBP+d08ftE9OkA0XJsREud5TYqZJfJ+pZVr8If6OUSaj27/LGLPRNbtNmR31ZGzXFLx/c7eA9Hx5aWwHXN6uoWtVbSItNaa9ujeD2jRwLWJ1ysMS1WpDEuJnsPgh6i1hmKRSbHZJ4wT0aFYHn9zaQ7Np0NRhbTC6UxO0aeh+u6eujr1b1MPNvdLcZnCIJ19lSr99cgJeu74TNh65AIvZ6FpkStlTwJsTwuJc4pRlMdhpolLLUFJmw5C2DbH7pZEwGw0wm4yuGTziCSMlKdYVDLx1Qye8cm0HxESZcHk77YK7gW0aoHWjeKwVWpYrr7Rjo024s386jl8oQofkRLx+fSe8OKaD2wd1Ew+zFubf2xc/bz2Jh65og/8Ii6jd2qc5WjSojS6KomE9hl/aCEt2yhcZXP/0MNcCg84pkIPbNsAXaxwL1H16aw/844sN5Xs73rTi658mFIaWqQQP13ZNwbu/7UOnVMcJTpwVNWfiZbi0SQJ6vpoBAKrrnHRIScTW54cj89B53PbZOtnzK7P1r1zbAeN7NUOByloztS1m/PbwQBw4U4Aft5zAz9tO4m4Pa6F0b1YHd/ZPR2qdWJiMBqTWiUO3tCTUspg9FjjHWeRDZN2b1cXPDwzQ3N8sCz60Mx/eJMSYkauyTo4Y3ESZjOgt1Nz83509YTYZ8e9bumP6L7v8WkAyEJokxuKz291ruYiUGHyUE2MKX1dirWVxL/RSPqYaMeX99aTe+GHLcYzv1QwGg8F18lBjMhrQrrF60yNP61kYjQa8qrLao1pq+7ERbfFl5lFMHeZYxr17szqyE5Kv13LiujRiAFcmXGErK++Bipbk3q7KxNfLYPDcNvzdm7rgsz8OY/p1jtdC/PXNRsdQ0D9/2I53b+oCAHjmqktl9xcf+5Nbe+CHzcdxv4cZCV3T6qCrkCVxMhkNGNRGX4Gr0r9u7IyFW0/iZE6xa3mABvEWLJjSTzZuLxZ7Dr3Evc7JYDBgzbTLYS2TZMMMatOt69W2IPPpoa6T4KVNEpCSFItGCRZXDZXB4DjZttKYohwTZUKKUODsfC+IRcoPDWuDG3s4hseUQ2m1oh3DNNFmI1o1jMegNg1we7/mbrO+lL+j+Dc0Gg2uKeqe6pvEwMTXrIA4/KBVcKplyUOD0Ht6htt28cIm2mR0ZWMuFFgxoLXjfZSSFIv3bu6m+diBmo5LVFl8J5arFW1G0zqxOHahSJZO9+Sfoy/Bb7tOySrbxQ8zk9HzqJb4wdozvS56eZg9oJc/61moFdhOGdIKU4bIOySKV3a+fqBOHdYaD361Bbf0TpNtF3uSqM0y0ltw5kth2pguKRjTJUX1NpPRgFt6N8N13VJks4a0XHFpI4+BYrDEx0RhXM80FJXa8FfWRQwrP4bOipNwx5RE3DO4JVKSYmXvTXFYQMw4jemSjIxdp3FDD/XOm+JrEm02YsVjg2Xvn5/u649PVh/EI1doN4Bq1TAeL43tgAbCzJSUpFjMu6s3GsRHo5VKRg9w9Ja4uVearLYhJsqkOjTpjZ6iavkaJ/pGqF+4pj2yzhfKir59XZytcWIM4i1mt9oucdjF+X33Zvp+989u74GXFu7CWzcEbmiPqDJqfPDx0pj2mP3HYUy7sh0MBgO+Xp+FCb31zbD4x4AWspkSTrf1aYbjF4vQSWMqr1O3tCSM65mG9PpxlV7D4NquKZi/+Tgm+9Fy2ORhvFyLr1dQY7qkoHPTJLcOpq0a1sZ7N3d16y3w6rUdMfuPQ666Fi1Th7XGtxuOueon/CF2UXQWLuoJPPzhHN4b0V47YLm9b3PM+fOwrpk+sdEmj+PrBoPBNUUXAN64vhOyc4s1e9y88/cuKLXZdS8opqzV6JCSiHdv6qqxdwW1f2N9WqoH32/d0BnL95zGzb3SQroWjfi7aU2/V7pNKIadfl1HLNhyHJM8DAlpUZvRFCVczPj6cXF5u0YehyCJQs0g+XoJG2S5ublITExETk4OEhL8X0+hprHa7Nh/Oh/tGsf7HMgs2HLctST54ddGe9z3/9YcxrbjOXjtuk4ep3yKy2J7e8zKkiSpUsHbl5lH8dT8bQCA3S+NDOoJ7mJhKX7dno0rOzXRbIpmtdmx9VgOOjVN9HmdDQqcfafycMXbqwAE/z2s1PXFJbhQvg6U87lzi63o9PwSAMCaaZer1kgRVSVfzt8Bv7ybPn06vv/+e+zevRuxsbHo27cvXn/9dbRtyz78wRRlMrr1M9Drms7JsEsSOjdN8rrvhABMxwu0QK58qWw1H2hJcdG4qWeax32iTEbNxlIUOq0bxWPWuK4+rQcVKGpDtvEWM7o3qwOrzY5G8ZzSStVbwC+rVq5ciSlTpmDt2rVYunQprFYrhg8fjoKCgkA/FQWIwWDAtV2booWfrbQjCZsakejqzskBqcXylXPl1q5CIzGDwYDvJvfBD/f2091ojihcBTzz8euvv8p+njNnDho2bIiNGzdi4ED1RceIqpKYOAlkFoXIXw9f0QZd05LQO10e+BgMBp/rPYjCUdALTnNycgAAdeuqV2WXlJSgpKRiamtubm6wD4lCwGI2+jXzhogcM4lGeFiRmqi6C2o1m91ux9SpU9GvXz906NBBdZ/p06cjMTHR9ZWayoWEIsG8Sb3RISUBX03qXdWHQkREYSaowceUKVOwfft2fPXVV5r7TJs2DTk5Oa6vrKysYB4ShUi3tDpYeP8AWRfGcMUsNhFRaAVt2OW+++7DwoULsWrVKjRtqt6wCAAsFgssltBXkxMREVHVCHjwIUkS7r//fsyfPx8rVqxAenrVrDFARERE4SngwceUKVPw5ZdfYsGCBYiPj0d2djYAIDExEbGxbIpD4UdrHRIiIgqOgHc41ZqqOHv2bNx+++1e788Op1QVfth8HOn1a7mtjUJERPpUaYfTMOvWTqTL2K7qC80REVHgceEIIiIiCikGH0RERBRSDD6IiIgopBh8EBERUUgx+CAiIqKQYvBBREREIcXgg4iIiEKKwQcRERGFFIMPIiIiCikGH0RERBRSDD6IiIgopBh8EBERUUgx+CAiIqKQCviqtpXlXBU3Nze3io+EiIiI9HKet/Wsbh92wUdeXh4AIDU1tYqPhIiIiHyVl5eHxMREj/sYJD0hSgjZ7XacOHEC8fHxMBgMAX3s3NxcpKamIisrCwkJCQF97EjD10o/vlb68bXSj6+Vb/h66Res10qSJOTl5SE5ORlGo+eqjrDLfBiNRjRt2jSoz5GQkMA3p058rfTja6UfXyv9+Fr5hq+XfsF4rbxlPJxYcEpEREQhxeCDiIiIQqpGBR8WiwXPPfccLBZLVR9K2ONrpR9fK/34WunH18o3fL30C4fXKuwKTomIiCiy1ajMBxEREVU9Bh9EREQUUgw+iIiIKKQYfBAREVFIRXzwcc011yAtLQ0xMTFo0qQJJkyYgBMnTni8T3FxMaZMmYJ69eqhdu3auP7663Hq1KkQHXHVOHz4MO68806kp6cjNjYWLVu2xHPPPYfS0lKP9xs8eDAMBoPsa/LkySE66qrh72tVE99XAPDKK6+gb9++iIuLQ1JSkq773H777W7vq5EjRwb3QMOAP6+VJEl49tln0aRJE8TGxmLYsGHYt29fcA80DJw/fx7jx49HQkICkpKScOeddyI/P9/jfWrS59X777+P5s2bIyYmBr169cK6des87v/tt9+iXbt2iImJQceOHfHLL78E9fgiPvgYMmQIvvnmG+zZswf/+9//cODAAfztb3/zeJ+HHnoIP/30E7799lusXLkSJ06cwHXXXReiI64au3fvht1ux0cffYQdO3bg7bffxocffoinnnrK633vuusunDx50vX1xhtvhOCIq46/r1VNfF8BQGlpKW644Qbcc889Pt1v5MiRsvfVvHnzgnSE4cOf1+qNN97AzJkz8eGHHyIzMxO1atXCiBEjUFxcHMQjrXrjx4/Hjh07sHTpUixcuBCrVq3CpEmTvN6vJnxeff3113j44Yfx3HPPYdOmTejcuTNGjBiB06dPq+7/559/Yty4cbjzzjuxefNmjB07FmPHjsX27duDd5BSDbNgwQLJYDBIpaWlqrdfvHhRioqKkr799lvXtl27dkkApDVr1oTqMMPCG2+8IaWnp3vcZ9CgQdKDDz4YmgMKY95eK76vJGn27NlSYmKirn1vu+02acyYMUE9nnCm97Wy2+1S48aNpTfffNO17eLFi5LFYpHmzZsXxCOsWjt37pQASOvXr3dtW7RokWQwGKTjx49r3q+mfF717NlTmjJliutnm80mJScnS9OnT1fd/8Ybb5RGjx4t29arVy/p7rvvDtoxRnzmQ3T+/HnMnTsXffv2RVRUlOo+GzduhNVqxbBhw1zb2rVrh7S0NKxZsyZUhxoWcnJyULduXa/7zZ07F/Xr10eHDh0wbdo0FBYWhuDowou314rvK9+tWLECDRs2RNu2bXHPPffg3LlzVX1IYefQoUPIzs6Wva8SExPRq1eviH5frVmzBklJSejRo4dr27Bhw2A0GpGZmenxvpH+eVVaWoqNGzfK3hNGoxHDhg3TfE+sWbNGtj8AjBgxIqjvobBbWC4YnnjiCbz33nsoLCxE7969sXDhQs19s7OzER0d7Tbe2qhRI2RnZwf5SMPH/v37MWvWLLz11lse97v55pvRrFkzJCcnY+vWrXjiiSewZ88efP/99yE60qqn57Xi+8o3I0eOxHXXXYf09HQcOHAATz31FEaNGoU1a9bAZDJV9eGFDed7p1GjRrLtkf6+ys7ORsOGDWXbzGYz6tat6/H3rgmfV2fPnoXNZlN9T+zevVv1PtnZ2SF/D1XLzMeTTz7pVjSk/BJf5MceewybN2/GkiVLYDKZcOutt0KqIY1dfX2tAOD48eMYOXIkbrjhBtx1110eH3/SpEkYMWIEOnbsiPHjx+OLL77A/PnzceDAgWD+WkER7NcqkvjzWvnipptuwjXXXIOOHTti7NixWLhwIdavX48VK1YE7pcIkWC/VpEk2K9VJH1eVXfVMvPxyCOP4Pbbb/e4T4sWLVzf169fH/Xr10ebNm1wySWXIDU1FWvXrkWfPn3c7te4cWOUlpbi4sWLsqvUU6dOoXHjxoH6FULG19fqxIkTGDJkCPr27YuPP/7Y5+fr1asXAEc2oGXLlj7fvyoF87Wq6e+rymrRogXq16+P/fv3Y+jQoQF73FAI5mvlfO+cOnUKTZo0cW0/deoUunTp4tdjViW9r1Xjxo3diifLyspw/vx5n/49VefPKy3169eHyWRym0nn6bOmcePGPu0fCNUy+GjQoAEaNGjg133tdjsAoKSkRPX27t27IyoqChkZGbj++usBAHv27MHRo0dVg5Vw58trdfz4cQwZMgTdu3fH7NmzYTT6nhjbsmULAMg+CKuLYL5WNfl9FQjHjh3DuXPnIv595av09HQ0btwYGRkZrmAjNzcXmZmZPs8uCgd6X6s+ffrg4sWL2LhxI7p37w4AWLZsGex2uyug0KM6f15piY6ORvfu3ZGRkYGxY8cCcJz3MjIycN9996nep0+fPsjIyMDUqVNd25YuXRrcz6aglbKGgbVr10qzZs2SNm/eLB0+fFjKyMiQ+vbtK7Vs2VIqLi6WJEmSjh07JrVt21bKzMx03W/y5MlSWlqatGzZMmnDhg1Snz59pD59+lTVrxESx44dk1q1aiUNHTpUOnbsmHTy5EnXl7iP+Frt379fevHFF6UNGzZIhw4dkhYsWCC1aNFCGjhwYFX9GiHhz2slSTXzfSVJknTkyBFp8+bN0gsvvCDVrl1b2rx5s7R582YpLy/PtU/btm2l77//XpIkScrLy5MeffRRac2aNdKhQ4ek3377TerWrZvUunVr17/bSOXrayVJkvTaa69JSUlJ0oIFC6StW7dKY8aMkdLT06WioqKq+BVCZuTIkVLXrl2lzMxM6ffff5dat24tjRs3znV7Tf68+uqrrySLxSLNmTNH2rlzpzRp0iQpKSlJys7OliRJkiZMmCA9+eSTrv3/+OMPyWw2S2+99Za0a9cu6bnnnpOioqKkbdu2Be0YIzr42Lp1qzRkyBCpbt26ksVikZo3by5NnjxZOnbsmGufQ4cOSQCk5cuXu7YVFRVJ9957r1SnTh0pLi5Ouvbaa2Unlkg0e/ZsCYDql5PytTp69Kg0cOBA1+vbqlUr6bHHHpNycnKq6LcIDX9eK0mqme8rSXJMm1V7rcTXBoA0e/ZsSZIkqbCwUBo+fLjUoEEDKSoqSmrWrJl01113uT44I5mvr5UkOabbPvPMM1KjRo0ki8UiDR06VNqzZ0/oDz7Ezp07J40bN06qXbu2lJCQIE2cOFEWpNX0z6tZs2ZJaWlpUnR0tNSzZ09p7dq1rtsGDRok3XbbbbL9v/nmG6lNmzZSdHS01L59e+nnn38O6vEZJKmGVF4SERFRWKiWs12IiIio+mLwQURERCHF4IOIiIhCisEHERERhRSDDyIiIgopBh9EREQUUgw+iIiIKKQYfBAREVFIMfggIiKikGLwQURERCHF4IOIiIhCisEHERERhdT/A0yUuacC5LMHAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW, can apply our ideal Learning Rate (0.1), add far more steps (e.g. 10k, repeated 2 or 3 times to see if Loss continues to go down)\n",
        "# Once Loss seems to have converged, and plateaus, can \"Learning Rate Decay\" by dropping Learning Rate a bit, to something like 0.01 from 0.1 (e.g. drop by factor 10) -> Potentially improves Loss even more\n",
        "\n",
        "MINI_BATCH_SIZE = 32\n",
        "\n",
        "for i in range( 10000 ): # Can crank up higher now (e.g. 10000)\n",
        "\n",
        "  # Create Mini Batch\n",
        "  ix = torch.randint( 0, X.shape[0], (MINI_BATCH_SIZE,) )\n",
        "  embeddings = C[X[ix]] # 32x3 x 2\n",
        "\n",
        "\n",
        "  # Hidden Layer\n",
        "  h = torch.tanh( embeddings.view(-1, 3*2) @ W1 + b1 ) # 32 x 100\n",
        "\n",
        "\n",
        "  # Output Layer\n",
        "  logits = h @ W2 + b2 # 32 x 27\n",
        "  loss = F.cross_entropy( logits, Y[ix] );\n",
        "\n",
        "\n",
        "  # Backward Propagation\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  #learningRate = 0.1 # The Ideal Larning Rate we saw -- RAN WITH 2-3 times\n",
        "  learningRate = 0.01 # Learning Rate Decay  (10x lower it) -- RAN WITH 2-3 times\n",
        "  for p in parameters:\n",
        "    p.data += -learningRate * p.grad\n",
        "\n",
        "\n",
        "# Check Resulting Loss\n",
        "embeddings = C[X] # 32x3 x 2\n",
        "h = torch.tanh( embeddings.view(-1, 6) @ W1 + b1 ) # 32 x 100\n",
        "logits = h @ W2 + b2 # 32 x 27\n",
        "loss = F.cross_entropy( logits, Y )\n",
        "print( loss.item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu0Yg6BSD1XL",
        "outputId": "640ff0c3-1456-40a1-c463-2c88dce5b45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3155136108398438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= DUPLICATED FOR CONVENIENCE =========================\n",
        "# ==============================================================================\n",
        "# =================================== RESET ====================================\n",
        "# ==============================================================================\n",
        "# Quick Re-Init of Model Parameters\n",
        "\n",
        "C = torch.randn( (27,2), generator=g )\n",
        "\n",
        "hiddenLayerNeuronCount = 100 # HyperParam\n",
        "W1 = torch.randn( (3*2, hiddenLayerNeuronCount), generator=g ) # Input = hiddenLayerNeuronCount, Output = 3*2 (3 Neurons, each with 2 Embedded Values)\n",
        "b1 = torch.randn( hiddenLayerNeuronCount )\n",
        "\n",
        "W2 = torch.randn( (hiddenLayerNeuronCount, 27), generator=g )\n",
        "b2 = torch.randn( 27, generator=g )\n",
        "\n",
        "\n",
        "# Ensure all Params Require Gradient (to be calc by Torch) -- Repeating here for Convenience.\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "MUHqDXsKITKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - NOTE: This Loss gets down below 2.45 from original BiGram Model we had, now we're below 2.4 (doing better than BiGram Model)\n",
        "# -- This isn't necessarily a better Model than the BiGram Model we had since 3481 Param Model (like we have here) is still a pretty small Model.\n",
        "# ---- And as the Capacity or # Parameters grows, it becomes more capable of OVER FITTING Training Set -> I.e. LOWER LOSS, but just from memorizing Training Set (Loss on unseen Data can be very large)\n",
        "# ---- This is where Validation Sets comes in\n",
        "# -- This Model has sum( p.nelement() for p in parameters ) many Parameters (each C, W1, b1, W2, b2 have their own values, accumlating to total # Params, here it's 3481)\n",
        "\n",
        "# Training Split (usually 80% of DataSet), Dev/Validation Split (10% of DataSet), Test Split (10% of DataSet)\n",
        "# -- Training Set used to Optimize Model Parameters\n",
        "# -- Validation Set used to \"develop\" the Model (tune Hyper Parameters, e.g. size of Hidden Layer, size of Embedding, etc.)\n",
        "# ---- Can try variations of Hyper Parameters to find best\n",
        "# -- Test Set used to evaluate Model Performance\n",
        "# ---- Done sparingly and a few times! If do too much, Model starts to learn Test Split and it becomes less useful (risk Overfitting to Test Split)\n",
        "\n",
        "\n",
        "# ============================== CHECKPOINT ====================================\n",
        "\n",
        "# =========================== Data Set Splits ==================================\n",
        "# NOW Split the Data into Training, Validation, and Test Splits\n",
        "\n",
        "# Takes list of Words and builds Xs and Ys for those Words only\n",
        "def build_dataset( words ):\n",
        "  block_size = 3\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append( context )\n",
        "      Y.append( ix )\n",
        "\n",
        "      context = context[1:] + [ix] # Shift Context Window\n",
        "\n",
        "  X = torch.tensor( X )\n",
        "  Y = torch.tensor( Y )\n",
        "  return X, Y\n",
        "\n",
        "# Shuffle all the Words in our DataSet\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle( words )\n",
        "\n",
        "n1 = int( 0.8 * len(words) ) # n1 is first 80% of the Words\n",
        "n2 = int( 0.9 * len(words) ) # n2 is next 90% of the Words (acts as 10% when used as range n1 to)\n",
        "\n",
        "X_train, Y_train = build_dataset( words[:n1] )\n",
        "X_dev, Y_dev = build_dataset( words[n1:n2] )\n",
        "X_test, Y_test = build_dataset( words[n2:] )\n",
        "\n"
      ],
      "metadata": {
        "id": "HCJNixfyDud5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now put the Data Set Splits to use\n",
        "\n",
        "# MODEL TRAINING -- Only uses Training Split\n",
        "# NOTE: Usually not done in-line, usually launch bunch of Jobs and wait for them to finish (can often take multiple days with larger Models)\n",
        "\n",
        "\n",
        "MINI_BATCH_SIZE = 32\n",
        "\n",
        "for i in range( 10000 ): # Increasing Steps for faster training in one go (was 30k for initial, then dropped to 10k when Learning Rate Decayed, mainly for faster training)\n",
        "\n",
        "  # Create Mini Batch\n",
        "  ix = torch.randint( 0, X_train.shape[0], (MINI_BATCH_SIZE,) )\n",
        "  embeddings = C[X_train[ix]] # 32x3 x 2\n",
        "\n",
        "\n",
        "  # Hidden Layer\n",
        "  h = torch.tanh( embeddings.view(-1, 3*2) @ W1 + b1 ) # 32 x 100\n",
        "\n",
        "\n",
        "  # Output Layer\n",
        "  logits = h @ W2 + b2 # 32 x 27\n",
        "  loss = F.cross_entropy( logits, Y_train[ix] );\n",
        "\n",
        "\n",
        "  # Backward Propagation\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  #learningRate = 0.1 # The Ideal Larning Rate we saw -- RAN WITH 2-3 times\n",
        "  learningRate = 0.01 # Learning Rate Decay  (10x lower it) -- RAN WITH 2-3 times\n",
        "  for p in parameters:\n",
        "    p.data += -learningRate * p.grad\n",
        "\n",
        "print( loss.item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seh5joLPIFrl",
        "outputId": "169f292d-057f-4f77-8c46-9bee172d9260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2914910316467285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL EVALUATING -- Using dev set here...\n",
        "\n",
        "embeddings = C[X_dev] # 32x3 x 2\n",
        "h = torch.tanh( embeddings.view(-1, 6) @ W1 + b1 ) # 32 x 100\n",
        "logits = h @ W2 + b2 # 32 x 27\n",
        "loss = F.cross_entropy( logits, Y_dev )\n",
        "\n",
        "print( \"Dev Loss:\", loss.item() ) # Still pretty good Loss even know Model never seen these Examples yet\n",
        "\n",
        "\n",
        "# Can compare to Training Loss to make sure we're not Overfitting (they're about equal, which is good)\n",
        "# -- Model isn't powerful enough to Overfit the Data right now (to purely memorize data)\n",
        "# -- We're right now \"Underfitting\" since Training Loss and Dev/Test Losses are roughly equal\n",
        "# ---- Means our Model is very small. We can improve Performance by increasing Model size\n",
        "\n",
        "embeddings = C[X_train] # 32x3 x 2\n",
        "h = torch.tanh( embeddings.view(-1, 6) @ W1 + b1 ) # 32 x 100\n",
        "logits = h @ W2 + b2 # 32 x 27\n",
        "loss = F.cross_entropy( logits, Y_train )\n",
        "\n",
        "print( \"Train Loss:\", loss.item() ) # Still pretty good Loss even know Model never seen these Examples ye\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5mu-4K7JPXP",
        "outputId": "e1a31b7a-bd42-4fd5-d760-ff51a16cc6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.263089179992676\n",
            "2.2664990425109863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= INCREASING PARAMETER COUNT =========================\n",
        "# ==============================================================================\n",
        "# =================================== RESET ====================================\n",
        "# ==============================================================================\n",
        "\n",
        "# INCREASING MODEL SIZE (more Parameters)\n",
        "# -- NOTE: With a larger Model, will take longer to Converge (more Steps & thus Training Time)\n",
        "# -- NOTE: Bottlenecks can hinder Model's Performance growth (e.g. 2D Embeddings as Input may bottleneck Model from being able to represent much with them and keep from improving Performance)\n",
        "\n",
        "C = torch.randn( (27,2), generator=g )\n",
        "\n",
        "#hiddenLayerNeuronCount = 100 # HyperParam\n",
        "hiddenLayerNeuronCount = 300 # HyperParam -- BUMPING UP to increase Model Size (~10k Params instead of ~3k Params)\n",
        "W1 = torch.randn( (3*2, hiddenLayerNeuronCount), generator=g ) # Input = hiddenLayerNeuronCount, Output = 3*2 (3 Neurons, each with 2 Embedded Values)\n",
        "b1 = torch.randn( hiddenLayerNeuronCount )\n",
        "\n",
        "W2 = torch.randn( (hiddenLayerNeuronCount, 27), generator=g )\n",
        "b2 = torch.randn( 27, generator=g )\n",
        "\n",
        "\n",
        "# Ensure all Params Require Gradient (to be calc by Torch) -- Repeating here for Convenience.\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "\n",
        "print( sum(p.nelement() for p in parameters) ) # Number of Parameters in Total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlZB1IccKiSS",
        "outputId": "7a87bcb0-f2a0-43ec-aa2e-74fc7e6caab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10281\n"
          ]
        }
      ]
    }
  ]
}